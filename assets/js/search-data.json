{
  
    
        "post0": {
            "title": "Serving A Machine Learning Model With Api",
            "content": "Till now, all the blogs I’ve posted were basically leveraging CNN pre-tarined models to develop a model that was useful for our given use case. But from my experience at TCS Rapid Labs, I have learnt that training models is not enough. We not to serve them to make them usable across our production systems. . Now, once we have acknowledged this gap in our learning journey, there is one more thing we need to take account of. Most of the production grade systems and legacy application who are willing to leavrage the power of AI are essentially not written in Python. However, most of us are using Python to train our models. This is where we can use power of APIs. . In this blog, we will be learning to wrap our ML Models into an API. We will be building a simple classifier whose job is to classify a breast tumour as malignant or benign based on the features in the dataset. After that we will be saving the trained model(Serialization &amp; Deserialization), exposing the functionality of the model as an API and testing the API using Postman. Additionally the entire setup is avialble as a Docker Image here and code is availble here. . The model training script is uploaded at the GitHub Repository above and will not be discussed here. However, we will discuss about serializing the model as pickle file, deserializing it and exposing the functionality of the model as a Flask API. We will be using sk-learn’s joblib library for this. . import joblib joblib.dump(model, &#39;breast_cancer_knn_model.pkl&#39;) model_columns = list(cancer_data[&#39;feature_names&#39;]) joblib.dump(model_columns, &#39;model_columns.pkl&#39;) .",
            "url": "https://saptarshidatta.in/2021/12/14/Serving-a-Machine-Learning-model-with-API.html",
            "relUrl": "/2021/12/14/Serving-a-Machine-Learning-model-with-API.html",
            "date": " • Dec 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Basic CNN and MLP implementation using PyTorch",
            "content": "This is a basic implementation of CNN and MLP in the PyTorch framework. We haven&#39;t done anything fancy with the NN architecture. However, make sure to check the preprocessing area. . We have used the coarse lebels of Cifar-100 and also implemented a sub-routine to determine the mean and standard deviation per channel of the input CIFAR-100 data. This approach will be much better than guessing and searching Research Papers for the appropriate digits. . Other than this, we have provided the user with an option to run either MLP implementation or CNN implementation. . Importing Libraries . import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt from torchvision.datasets import CIFAR100 from sklearn.metrics import confusion_matrix . Preprocessing Area . class CIFAR100coarse(CIFAR100): def __init__(self, root, train = True, transform = None, target_transform=None, download = False): super(CIFAR100coarse, self).__init__(root, train, transform, target_transform, download) coarse_labels = np.array([ 4, 1, 14, 8, 0, 6, 7, 7, 18, 3, 3, 14, 9, 18, 7, 11, 3, 9, 7, 11, 6, 11, 5, 10, 7, 6, 13, 15, 3, 15, 0, 11, 1, 10, 12, 14, 16, 9, 11, 5, 5, 19, 8, 8, 15, 13, 14, 17, 18, 10, 16, 4, 17, 4, 2, 0, 17, 4, 18, 17, 10, 3, 2, 12, 12, 16, 12, 1, 9, 19, 2, 10, 0, 1, 16, 12, 9, 13, 15, 13, 16, 19, 2, 4, 6, 19, 5, 5, 8, 19, 18, 1, 2, 15, 6, 0, 17, 8, 14, 13]) self.targets = coarse_labels[self.targets] self.classes = [[&#39;beaver&#39;, &#39;dolphin&#39;, &#39;otter&#39;, &#39;seal&#39;, &#39;whale&#39;], [&#39;aquarium_fish&#39;, &#39;flatfish&#39;, &#39;ray&#39;, &#39;shark&#39;, &#39;trout&#39;], [&#39;orchid&#39;, &#39;poppy&#39;, &#39;rose&#39;, &#39;sunflower&#39;, &#39;tulip&#39;], [&#39;bottle&#39;, &#39;bowl&#39;, &#39;can&#39;, &#39;cup&#39;, &#39;plate&#39;], [&#39;apple&#39;, &#39;mushroom&#39;, &#39;orange&#39;, &#39;pear&#39;, &#39;sweet_pepper&#39;], [&#39;clock&#39;, &#39;keyboard&#39;, &#39;lamp&#39;, &#39;telephone&#39;, &#39;television&#39;], [&#39;bed&#39;, &#39;chair&#39;, &#39;couch&#39;, &#39;table&#39;, &#39;wardrobe&#39;], [&#39;bee&#39;, &#39;beetle&#39;, &#39;butterfly&#39;, &#39;caterpillar&#39;, &#39;cockroach&#39;], [&#39;bear&#39;, &#39;leopard&#39;, &#39;lion&#39;, &#39;tiger&#39;, &#39;wolf&#39;], [&#39;bridge&#39;, &#39;castle&#39;, &#39;house&#39;, &#39;road&#39;, &#39;skyscraper&#39;], [&#39;cloud&#39;, &#39;forest&#39;, &#39;mountain&#39;, &#39;plain&#39;, &#39;sea&#39;], [&#39;camel&#39;, &#39;cattle&#39;, &#39;chimpanzee&#39;, &#39;elephant&#39;, &#39;kangaroo&#39;], [&#39;fox&#39;, &#39;porcupine&#39;, &#39;possum&#39;, &#39;raccoon&#39;, &#39;skunk&#39;], [&#39;crab&#39;, &#39;lobster&#39;, &#39;snail&#39;, &#39;spider&#39;, &#39;worm&#39;], [&#39;baby&#39;, &#39;boy&#39;, &#39;girl&#39;, &#39;man&#39;, &#39;woman&#39;], [&#39;crocodile&#39;, &#39;dinosaur&#39;, &#39;lizard&#39;, &#39;snake&#39;, &#39;turtle&#39;], [&#39;hamster&#39;, &#39;mouse&#39;, &#39;rabbit&#39;, &#39;shrew&#39;, &#39;squirrel&#39;], [&#39;maple_tree&#39;, &#39;oak_tree&#39;, &#39;palm_tree&#39;, &#39;pine_tree&#39;, &#39;willow_tree&#39;], [&#39;bicycle&#39;, &#39;bus&#39;, &#39;motorcycle&#39;, &#39;pickup_truck&#39;, &#39;train&#39;], [&#39;lawn_mower&#39;, &#39;rocket&#39;, &#39;streetcar&#39;, &#39;tank&#39;, &#39;tractor&#39;]] . def mean_std(): transform = transforms.Compose([transforms.ToTensor()]) data_set = CIFAR100coarse(root = &#39;./data&#39;, train = True, transform = transform, target_transform=None, download = True) train_loader = torch.utils.data.DataLoader(data_set, batch_size = 4, shuffle=True) images, labels = iter(train_loader).next() numpy_images = images.numpy() per_image_mean = np.mean(numpy_images, axis=(2,3)) per_image_std = np.std(numpy_images, axis=(2,3)) per_channel_mean = np.mean(per_image_mean, axis=0) per_channel_std = np.mean(per_image_std, axis=0) return per_channel_mean, per_channel_std . def cifar_preprocessor(batch_size, test_shuffle = False): per_channel_mean, per_channel_std = mean_std() transform =transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean = per_channel_mean, std = per_channel_std)]) train_set = CIFAR100coarse(root = &#39;./data&#39;, train = True, transform = transform, target_transform=None, download = True) train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True) test_set = CIFAR100coarse(root = &#39;./data&#39;, train = True, transform = transform, target_transform=None, download = True) test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = test_shuffle) return train_loader, test_loader, train_set, test_set . MLP implementation . class MLP_network(nn.Module): def __init__(self): &#39;&#39;&#39; Check this link for nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html &#39;&#39;&#39; super(MLP_network, self).__init__() self.fc1 = torch.nn.Linear(3072, 2000) self.fc2 = torch.nn.Linear(2000, 1500) self.fc3 = torch.nn.Linear(1500, 1000) self.fc4 = torch.nn.Linear(1000, 800) self.fc5 = torch.nn.Linear(800, 500) self.fc6 = torch.nn.Linear(500, 200) self.out = torch.nn.Linear(200, 20) def forward(self, t): #Layer1 t = F.relu(self.fc1(t)) #Layer2 t = F.relu(self.fc2(t)) #Layer3 t = F.relu(self.fc3(t)) #Layer4 t = F.relu(self.fc4(t)) #Layer5 t = F.relu(self.fc5(t)) #Layer6 t = F.relu(self.fc6(t)) #Layer7 t = self.out(t) return t . CNN Implementation . class CNN_network(nn.Module): def __init__(self): super(CNN_network,self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.fc1 = nn.Linear(in_features=16*5*5, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=84) self.out = nn.Linear(in_features=84, out_features=20) def forward(self, t): #Layer 1 t = t #Layer 2 t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (6,14,14) #Layer 3 t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (16,5,5) #Layer 4 t = t.reshape(-1, 16*5*5) t = self.fc1(t) t = F.relu(t)#output shape : (1,120) #Layer 5 t = self.fc2(t) t = F.relu(t)#output shape : (1, 84) #Layer 6/ Output Layer t = self.out(t)#output shape : (1, 20) return t . Training Module . def train_nn(model, train_loader, optimizer, i): print(&#39;&gt;&gt;&gt; Training Start &gt;&gt;&gt;&#39;) for epoch in range(30): total_loss = 0 total_correct = 0 for batch in train_loader: images, labels = batch if i == 1: images = images.reshape(-1, 32*32*3) predictions = model(images) loss = F.cross_entropy(predictions, labels) optimizer.zero_grad() loss.backward() optimizer.step() total_loss = total_loss + loss.item() total_correct = total_correct + predictions.argmax(dim=1).eq(labels).sum().item() print(&#39;epoch:&#39;, epoch, &quot;total_correct:&quot;, total_correct, &quot;loss:&quot;, total_loss) print(&#39;&gt;&gt;&gt; Training Complete &gt;&gt;&gt;&#39;) . Prediction Module . @torch.no_grad() def get_all_preds(model, loader, i): all_preds = torch.tensor([]) for batch in loader: images, labels = batch if i == 1: images = images.reshape(-1, 32*32*3) preds = model(images) all_preds = torch.cat((all_preds, preds) ,dim=0) return all_preds . Testing Module . def calc_accuracy(test_preds, test_set, i): actual_labels = torch.Tensor(test_set.targets) preds_correct = test_preds.argmax(dim=1).eq(actual_labels).sum().item() if i == 1: print(&#39;Multi Layer Perceptrons&#39;) else: print(&#39;Convolutional Neural Networks&#39;) print(&#39;total correct:&#39;, preds_correct) print(&#39;accuracy:&#39;, preds_correct / len(test_set)) . Implementation Module . def main(): train_loader, test_loader, train_set, test_set = cifar_preprocessor(64) print(&#39;Enter 1 for MLP, 2 for CNN&#39;) i = int(input()) if i == 1: print(&#39;Multiple Layer of Perceptrons&#39;) model = MLP_network() optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9) train_nn(model, train_loader, optimizer, i) all_preds = get_all_preds(model, test_loader, i) calc_accuracy(all_preds, test_set, i) PATH = &#39;./cifar100_mlp.pth&#39; torch.save(model.state_dict(), PATH) elif i == 2: print(&#39;Convolutional Neural Network&#39;) model = CNN_network() optimizer = optim.SGD(model.parameters(), lr=0.001, momentum = 0.9) train_nn(model, train_loader, optimizer, i) all_preds = get_all_preds(model, test_loader, i) calc_accuracy(all_preds, test_set, i) PATH = &#39;./cifar100_cnn.pth&#39; torch.save(model.state_dict(), PATH) else: print(&#39;Wrong Choice...Try Again!!!&#39;) . main() . Files already downloaded and verified Files already downloaded and verified Files already downloaded and verified Enter 1 for MLP, 2 for CNN 1 Multiple Layer of Perceptrons &gt;&gt;&gt; Training Start &gt;&gt;&gt; epoch: 0 total_correct: 2817 loss: 2342.371166229248 epoch: 1 total_correct: 3320 loss: 2340.0113854408264 epoch: 2 total_correct: 4354 loss: 2332.8723883628845 epoch: 3 total_correct: 4828 loss: 2258.425218820572 epoch: 4 total_correct: 6902 loss: 2124.3050644397736 epoch: 5 total_correct: 8667 loss: 2058.6139965057373 epoch: 6 total_correct: 10074 loss: 1992.6582174301147 epoch: 7 total_correct: 11448 loss: 1925.155211687088 epoch: 8 total_correct: 12894 loss: 1862.4140621423721 epoch: 9 total_correct: 13918 loss: 1809.2699065208435 epoch: 10 total_correct: 14919 loss: 1759.2945556640625 epoch: 11 total_correct: 16024 loss: 1704.545359969139 epoch: 12 total_correct: 16870 loss: 1657.445464372635 epoch: 13 total_correct: 17835 loss: 1607.2123988866806 epoch: 14 total_correct: 18817 loss: 1554.9647988080978 epoch: 15 total_correct: 19625 loss: 1504.4832880496979 epoch: 16 total_correct: 20438 loss: 1457.3500571250916 epoch: 17 total_correct: 21534 loss: 1401.9261798858643 epoch: 18 total_correct: 22502 loss: 1346.6949373483658 epoch: 19 total_correct: 23500 loss: 1293.0633860826492 epoch: 20 total_correct: 24571 loss: 1238.7428677082062 epoch: 21 total_correct: 25787 loss: 1173.1022816300392 epoch: 22 total_correct: 26799 loss: 1117.0504439473152 epoch: 23 total_correct: 28132 loss: 1050.3701268434525 epoch: 24 total_correct: 29300 loss: 991.4318377971649 epoch: 25 total_correct: 30446 loss: 928.4249439835548 epoch: 26 total_correct: 31757 loss: 865.1841924786568 epoch: 27 total_correct: 33180 loss: 796.4853633642197 epoch: 28 total_correct: 34014 loss: 754.7700568437576 epoch: 29 total_correct: 35392 loss: 689.5142071247101 &gt;&gt;&gt; Training Complete &gt;&gt;&gt; Multi Layer Perceptrons total correct: 36676 accuracy: 0.73352 .",
            "url": "https://saptarshidatta.in/2021/06/13/mlp_cnn_cifar100.html",
            "relUrl": "/2021/06/13/mlp_cnn_cifar100.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "CT Scan COVID-19 classifier using PyTorch",
            "content": "Recently, the UC San Diego open sourced a dataset containing lung CT Scan images of COVID-19 patients, the first of its kind in the public domain. In this post we will use PyTorch to build a classifier that takes the lung CT scan of a patient and classifies it as COVID-19 positive or negative. . However, let me add a disclaimer at the very beginning: . Let it be known that I am not a medical or radiology professional. My experience lies in machine learning, deep learning, and computer vision. Whatever I write comes from my limited knowledge of medical imaging. The results from this post should not be used in clinical studies or may be used at your own risk. I expect the code should be used to develop more robust classifiers that can be deployed in the real world. . I have followed several examples to arrive at this result, references to which is published at the end of this post. . Now, without wasting much time, let&#39;s jump into the code. We start by importing the libraries. . import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader from torchvision import transforms as transforms from skimage.util import montage import os import cv2 import random import matplotlib.pyplot as plt import torch.optim as optim from PIL import Image from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix import glob import shutil import numpy as np from torchvision.models import vgg19_bn import numpy as np import seaborn as sns random.seed(0) device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot; . We will start by cloning the GitHub repository to obtain the data along with all other information that we will need to process the data. . Please access the GitHub repository here. . !git clone https://github.com/UCSD-AI4H/COVID-CT . Cloning into &#39;COVID-CT&#39;... remote: Enumerating objects: 5459, done. remote: Total 5459 (delta 0), reused 0 (delta 0), pack-reused 5459 Receiving objects: 100% (5459/5459), 1.09 GiB | 22.39 MiB/s, done. Resolving deltas: 100% (360/360), done. Checking out files: 100% (1048/1048), done. . !unzip /content/COVID-CT/Images-processed/CT_COVID.zip; !unzip /content/COVID-CT/Images-processed/CT_NonCOVID.zip; . Dataset Description . Before we start building the classifier, let us discuss a bit about the dataset that we have with us. We have a positive class with CT Scans of COVID-19 positive patients where as the negative class contains CT Scans of patients suffering from other ailments. . We must consider the facr that a doctor will never send a healthy patient for a CT Scan. In order to builda robust classifier, we must be aware that almost all the patients in our dataset will be suffering from some respiratory illness/ pneumonia. . Therefore, our classifier should be able to distinguish between COVID-19 induced pneumonia and other non-COVID-19 related respiratory illness/ pneumonia. . covid_files_path = &#39;/content/CT_COVID&#39; covid_files = [os.path.join(covid_files_path, x) for x in os.listdir(covid_files_path)] covid_images = [cv2.imread(x) for x in random.sample(covid_files, 5)] plt.figure(figsize=(20,10)) columns = 5 for i, image in enumerate(covid_images): plt.subplot(len(covid_images) / columns + 1, columns, i + 1) plt.imshow(image) . non_covid_files_path = &#39;/content/CT_NonCOVID&#39; non_covid_files = [os.path.join(non_covid_files_path, x) for x in os.listdir(non_covid_files_path)] non_covid_images = [cv2.imread(x) for x in random.sample(non_covid_files, 5)] plt.figure(figsize=(20,10)) columns = 5 for i, image in enumerate(non_covid_images): plt.subplot(len(covid_images) / columns + 1, columns, i + 1) plt.imshow(image) . Loading Data . The entire dataset is divided into two 3 Splits: The train set, the test set and the validation set. The dataset details are described in this pre print: COVID-CT-Dataset: A CT Scan Dataset about COVID-19 . Details about the split is present inside the Data-split folder. This folder basically contains test files that explains which files belong to which split. . We create a function called read_txt to read the text files inside the data-split folder. We then We then create the CovidCTScanDataset class which basically subclasses the torch.utils.data.Dataset class. . def read_txt(txt_path): with open(txt_path) as f: lines = f.readlines() txt_data = [line.strip() for line in lines] return txt_data class CovidCTScanDataset(Dataset): def __init__(self, root_dir, classes, covid_files, non_covid_files, transform=None): self.root_dir = root_dir self.classes = classes self.files_path = [non_covid_files, covid_files] self.transform = transform self.image_list = [] # read the files from data split text files covid_files = read_txt(covid_files) non_covid_files = read_txt(non_covid_files) # combine the positive and negative files into a cummulative files list for cls_index in range(len(self.classes)): class_files = [[os.path.join(self.root_dir, self.classes[cls_index], x), cls_index] for x in read_txt(self.files_path[cls_index])] self.image_list += class_files def __len__(self): return len(self.image_list) def __getitem__(self, idx): path = self.image_list[idx][0] # Read the image image = Image.open(path).convert(&#39;RGB&#39;) # Apply transforms if self.transform: image = self.transform(image) label = int(self.image_list[idx][1]) data = {&#39;img&#39;: image, &#39;label&#39;: label, &#39;paths&#39; : path} return data . The dataset returns a dictionary containing the image tensor, the label tensor, and a list of image paths included in the batch. . Image pre-processing and Data Augmentation . Here are the steps for Data pre-processing and augmentation: . A. Training set: . Resize the shorter side of the image to 256 while maintaining the aspect ratio | Randomly crop the image size ranging from 50% to 100% of the dimensions of the image. Finally, the crop is resized to 224 × 224 | Horizontally flip the image with a probability of 0.5 | Normalize the image to have 0 mean and standard deviation of 1 | B. Testing set . Resize the image to 224 × 224. | Normalize the image to have mean 0 and standard deviation of 1 | normalize = transforms.Normalize(mean=[0,0,0], std=[1,1,1]) train_transformer = transforms.Compose([ transforms.Resize(256), transforms.RandomResizedCrop((224),scale=(0.5,1.0)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize ]) val_transformer = transforms.Compose([ transforms.Resize((224,224)), transforms.ToTensor(), normalize ]) . We have defined the Dataset and DataLoader classes, and now we shall instantiate them. We will use the label 0 for the non-COVID cases, whereas we use 1 for the COVID positive cases. . batchsize = 64 trainset = CovidCTScanDataset(root_dir=&#39;/content/&#39;, classes = [&#39;CT_NonCOVID&#39;, &#39;CT_COVID&#39;], covid_files=&#39;/content/COVID-CT/Data-split/COVID/trainCT_COVID.txt&#39;, non_covid_files=&#39;/content/COVID-CT/Data-split/NonCOVID/trainCT_NonCOVID.txt&#39;, transform= train_transformer) valset = CovidCTScanDataset(root_dir=&#39;/content/&#39;, classes = [&#39;CT_NonCOVID&#39;, &#39;CT_COVID&#39;], covid_files=&#39;/content/COVID-CT/Data-split/COVID/valCT_COVID.txt&#39;, non_covid_files = &#39;/content/COVID-CT/Data-split/NonCOVID/valCT_NonCOVID.txt&#39;, transform= val_transformer) testset = CovidCTScanDataset(root_dir=&#39;/content/&#39;, classes = [&#39;CT_NonCOVID&#39;, &#39;CT_COVID&#39;], covid_files=&#39;/content/COVID-CT/Data-split/COVID/testCT_COVID.txt&#39;, non_covid_files=&#39;/content/COVID-CT/Data-split/NonCOVID/testCT_NonCOVID.txt&#39;, transform= val_transformer) train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True) val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False) test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False) . Performance Metrics . Simply evaluating our model based on accuracy will not be enough. This is because of all the people we test, we are going to have only a few patients will be COVID-19 positive. This is because of the strong lockdown measures undertaken by the different governments t tackle this crisis. . In this scenario, we need to compute various metrics like Sesitivity, Specificity and Area under ROC. Here is a description of the three different metrics: . Specificity : This metric will determine number of non-infected people that the model has classified as negative. . | Sensitivity : This metric will determine the total number of infected people that the model has predicted as positive. . | Area under ROC : It is a graph between Sensitivity and (1-Specificity). The area under the ROC curve can range from 0 to 1, where 1 represents the perfect classifier and 0.5 (meaning the curve follows the line y=x) represents a classifier which is as good as flipping a coin (random chance). An area under 0.5 means the classifier is even worse, and makes incorrect predictions more often than not. . | Hence, We write the function calc_metrics to compute these metrics and some other quantities that will be useful for analysis later. . def calc_metrics(model, test_loader, plot_roc_curve = False): model.eval() val_loss = 0 val_correct = 0 criterion = nn.CrossEntropyLoss() score_list = torch.Tensor([]).to(device) pred_list = torch.Tensor([]).to(device).long() target_list = torch.Tensor([]).to(device).long() path_list = [] for iter_num, data in enumerate(test_loader): # Convert image data into single channel data image, target = data[&#39;img&#39;].to(device), data[&#39;label&#39;].to(device) paths = data[&#39;paths&#39;] path_list.extend(paths) # Compute the loss with torch.no_grad(): output = model(image) # Log loss val_loss += criterion(output, target.long()).item() # Calculate the number of correctly classified examples pred = output.argmax(dim=1, keepdim=True) val_correct += pred.eq(target.long().view_as(pred)).sum().item() # Bookkeeping score_list = torch.cat([score_list, nn.Softmax(dim = 1)(output)[:,1].squeeze()]) pred_list = torch.cat([pred_list, pred.squeeze()]) target_list = torch.cat([target_list, target.squeeze()]) classification_metrics = classification_report(target_list.tolist(), pred_list.tolist(), target_names = [&#39;CT_NonCOVID&#39;, &#39;CT_COVID&#39;], output_dict= True) # sensitivity is the recall of the positive class sensitivity = classification_metrics[&#39;CT_COVID&#39;][&#39;recall&#39;] # specificity is the recall of the negative class specificity = classification_metrics[&#39;CT_NonCOVID&#39;][&#39;recall&#39;] # accuracy accuracy = classification_metrics[&#39;accuracy&#39;] # confusion matrix conf_matrix = confusion_matrix(target_list.tolist(), pred_list.tolist()) # roc score roc_score = roc_auc_score(target_list.tolist(), score_list.tolist()) # plot the roc curve if plot_roc_curve: fpr, tpr, _ = roc_curve(target_list.tolist(), score_list.tolist()) plt.plot(fpr, tpr, label = &quot;Area under ROC = {:.4f}&quot;.format(roc_score)) plt.legend(loc = &#39;best&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.show() # put together values metrics_dict = {&quot;Accuracy&quot;: accuracy, &quot;Sensitivity&quot;: sensitivity, &quot;Specificity&quot;: specificity, &quot;Roc_score&quot; : roc_score, &quot;Confusion Matrix&quot;: conf_matrix, &quot;Validation Loss&quot;: val_loss / len(test_loader), &quot;score_list&quot;: score_list.tolist(), &quot;pred_list&quot;: pred_list.tolist(), &quot;target_list&quot;: target_list.tolist(), &quot;paths&quot;: path_list} return metrics_dict . The Model . We use the pretrained VGG-19 with batch normalization as our model. We then replace its final linear layer with one having 2 neurons at its output, and perform transfer learning over our dataset. . model = vgg19_bn(pretrained=True); model.classifier[6] = nn.Linear(in_features=4096, out_features=2); model.to(device); . Downloading: &#34;https://download.pytorch.org/models/vgg19_bn-c79401a0.pth&#34; to /root/.cache/torch/hub/checkpoints/vgg19_bn-c79401a0.pth . . learning_rate = 0.01 optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9) . Early Stopping . We have implemented a class called EarlyStopping which is supposed to keep record of the moving average of loss and accuracy of the model during training. If the metric doesn&#39;t improve beyond a set number of epochs, defined by the variablepatience, then the method stop returns: . 0, if patience has not been exhausted for either accuracy or the loss . | 1, if patience is exhausted for both the accuracy and the loss . | 2, if patience has been exhausted only for accuracy . | 3, if patience has been exhausted only for loss . | . Please note that patience has been exhausted for a metric means that the metric has not been improving for a said number of epochs that is determined by the value we set for thepatience variable. . from collections import deque class EarlyStopping(object): def __init__(self, patience = 5): super(EarlyStopping, self).__init__() self.patience = patience self.previous_loss = int(1e8) self.previous_accuracy = 0 self.init = False self.accuracy_decrease_iters = 0 self.loss_increase_iters = 0 self.best_running_accuracy = 0 self.best_running_loss = int(1e7) def add_data(self, model, loss, accuracy): # compute moving average if not self.init: running_loss = loss running_accuracy = accuracy self.init = True else: running_loss = 0.2 * loss + 0.8 * self.previous_loss running_accuracy = 0.2 * accuracy + 0.8 * self.previous_accuracy # check if running accuracy has improved beyond the best running accuracy recorded so far if running_accuracy &lt; self.best_running_accuracy: self.accuracy_decrease_iters += 1 else: self.best_running_accuracy = running_accuracy self.accuracy_decrease_iters = 0 # check if the running loss has decreased from the best running loss recorded so far if running_loss &gt; self.best_running_loss: self.loss_increase_iters += 1 else: self.best_running_loss = running_loss self.loss_increase_iters = 0 # log the current accuracy and loss self.previous_accuracy = running_accuracy self.previous_loss = running_loss def stop(self): # compute thresholds accuracy_threshold = self.accuracy_decrease_iters &gt; self.patience loss_threshold = self.loss_increase_iters &gt; self.patience # return codes corresponding to exhuaustion of patience for either accuracy or loss # or both of them if accuracy_threshold and loss_threshold: return 1 if accuracy_threshold: return 2 if loss_threshold: return 3 return 0 def reset(self): # reset self.accuracy_decrease_iters = 0 self.loss_increase_iters = 0 early_stopper = EarlyStopping(patience = 2) . Training the Model . If the early_stopper object returns 3, which means the patience for loss is exhausted, we will multiply the learning_rate by 0.1 . | If the early_stopper object returns 1, which means patience for both loss and accuracy has been exhausted, we will stop the training. . | . The reasons for such a policy is because Cross Entropy loss prefers high confidence predictions. So a more accurate model which is less confident about its predictions may have a higher loss than the model with lower accuracy but very confident predictions. Therefore, we make the decision only to stop training the model when the accuracy stops increasing as well. . We also save the model with the highest accuracy with the name best_model.pkl . best_model = model best_val_score = 0 criterion = nn.CrossEntropyLoss() for epoch in range(60): model.train() train_loss = 0 train_correct = 0 for iter_num, data in enumerate(train_loader): image, target = data[&#39;img&#39;].to(device), data[&#39;label&#39;].to(device) # Compute the loss output = model(image) loss = criterion(output, target.long()) # Log loss train_loss += loss.item() loss.backward() # Perform gradient udpate optimizer.step() optimizer.zero_grad() # Calculate the number of correctly classified examples pred = output.argmax(dim=1, keepdim=True) train_correct += pred.eq(target.long().view_as(pred)).sum().item() # Compute and print the performance metrics metrics_dict = calc_metrics(model, val_loader) print(&#39; Epoch {} Iteration {}--&#39;.format(epoch, iter_num)) print(&quot;Accuracy t {:.3f}&quot;.format(metrics_dict[&#39;Accuracy&#39;])) print(&quot;Sensitivity t {:.3f}&quot;.format(metrics_dict[&#39;Sensitivity&#39;])) print(&quot;Specificity t {:.3f}&quot;.format(metrics_dict[&#39;Specificity&#39;])) print(&quot;Area Under ROC t {:.3f}&quot;.format(metrics_dict[&#39;Roc_score&#39;])) print(&quot;Val Loss t {}&quot;.format(metrics_dict[&quot;Validation Loss&quot;])) print(&quot;&quot;) # Save the model with best validation accuracy if metrics_dict[&#39;Accuracy&#39;] &gt; best_val_score: torch.save(model, &quot;best_model.pkl&quot;) best_val_score = metrics_dict[&#39;Accuracy&#39;] # print the metrics for training data for the epoch print(&#39; nTraining Performance Epoch {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%) n&#39;.format( epoch, train_loss/len(train_loader.dataset), train_correct, len(train_loader.dataset), 100.0 * train_correct / len(train_loader.dataset))) # Add data to the EarlyStopper object early_stopper.add_data(model, metrics_dict[&#39;Validation Loss&#39;], metrics_dict[&#39;Accuracy&#39;]) # If both accuracy and loss are not improving, stop the training if early_stopper.stop() == 1: break # if only loss is not improving, lower the learning rate if early_stopper.stop() == 3: for param_group in optimizer.param_groups: learning_rate *= 0.1 param_group[&#39;lr&#39;] = learning_rate print(&#39;Updating the learning rate to {}&#39;.format(learning_rate)) early_stopper.reset() . Epoch 0 Iteration 6-- Accuracy 0.525 Sensitivity 0.150 Specificity 0.914 Area Under ROC 0.544 Val Loss 1.0580639839172363 Training Performance Epoch 0: Average loss: 0.0101, Accuracy: 270/425 (64%) Epoch 1 Iteration 6-- Accuracy 0.686 Sensitivity 0.567 Specificity 0.810 Area Under ROC 0.769 Val Loss 0.7278739809989929 Training Performance Epoch 1: Average loss: 0.0065, Accuracy: 334/425 (79%) Epoch 2 Iteration 6-- Accuracy 0.695 Sensitivity 0.967 Specificity 0.414 Area Under ROC 0.803 Val Loss 1.1830564178526402 Training Performance Epoch 2: Average loss: 0.0049, Accuracy: 377/425 (89%) Epoch 3 Iteration 6-- Accuracy 0.746 Sensitivity 0.750 Specificity 0.741 Area Under ROC 0.827 Val Loss 0.6438769102096558 Training Performance Epoch 3: Average loss: 0.0049, Accuracy: 380/425 (89%) Epoch 4 Iteration 6-- Accuracy 0.814 Sensitivity 0.667 Specificity 0.966 Area Under ROC 0.887 Val Loss 0.5770577192306519 Training Performance Epoch 4: Average loss: 0.0038, Accuracy: 385/425 (91%) Epoch 5 Iteration 6-- Accuracy 0.847 Sensitivity 0.833 Specificity 0.862 Area Under ROC 0.902 Val Loss 0.44977009296417236 Training Performance Epoch 5: Average loss: 0.0023, Accuracy: 404/425 (95%) Epoch 6 Iteration 6-- Accuracy 0.780 Sensitivity 0.717 Specificity 0.845 Area Under ROC 0.868 Val Loss 0.6709117889404297 Training Performance Epoch 6: Average loss: 0.0016, Accuracy: 409/425 (96%) Epoch 7 Iteration 6-- Accuracy 0.737 Sensitivity 0.600 Specificity 0.879 Area Under ROC 0.862 Val Loss 0.945330023765564 Training Performance Epoch 7: Average loss: 0.0017, Accuracy: 408/425 (96%) Epoch 8 Iteration 6-- Accuracy 0.669 Sensitivity 0.350 Specificity 1.000 Area Under ROC 0.894 Val Loss 1.3250054828822613 Training Performance Epoch 8: Average loss: 0.0009, Accuracy: 413/425 (97%) Epoch 9 Iteration 6-- Accuracy 0.831 Sensitivity 0.733 Specificity 0.931 Area Under ROC 0.918 Val Loss 0.44354573637247086 Training Performance Epoch 9: Average loss: 0.0010, Accuracy: 417/425 (98%) Updating the learning rate to 0.001 Epoch 10 Iteration 6-- Accuracy 0.771 Sensitivity 0.850 Specificity 0.690 Area Under ROC 0.900 Val Loss 0.49736201763153076 Training Performance Epoch 10: Average loss: 0.0005, Accuracy: 421/425 (99%) Epoch 11 Iteration 6-- Accuracy 0.771 Sensitivity 0.917 Specificity 0.621 Area Under ROC 0.897 Val Loss 0.5688106715679169 Training Performance Epoch 11: Average loss: 0.0002, Accuracy: 422/425 (99%) Epoch 12 Iteration 6-- Accuracy 0.788 Sensitivity 0.883 Specificity 0.690 Area Under ROC 0.899 Val Loss 0.5555288046598434 Training Performance Epoch 12: Average loss: 0.0003, Accuracy: 422/425 (99%) Epoch 13 Iteration 6-- Accuracy 0.805 Sensitivity 0.850 Specificity 0.759 Area Under ROC 0.900 Val Loss 0.5532597452402115 Training Performance Epoch 13: Average loss: 0.0002, Accuracy: 425/425 (100%) Epoch 14 Iteration 6-- Accuracy 0.822 Sensitivity 0.817 Specificity 0.828 Area Under ROC 0.903 Val Loss 0.5708105862140656 Training Performance Epoch 14: Average loss: 0.0003, Accuracy: 424/425 (100%) Epoch 15 Iteration 6-- Accuracy 0.822 Sensitivity 0.817 Specificity 0.828 Area Under ROC 0.904 Val Loss 0.560054823756218 Training Performance Epoch 15: Average loss: 0.0005, Accuracy: 421/425 (99%) Epoch 16 Iteration 6-- Accuracy 0.805 Sensitivity 0.750 Specificity 0.862 Area Under ROC 0.905 Val Loss 0.6267178058624268 Training Performance Epoch 16: Average loss: 0.0003, Accuracy: 423/425 (100%) Epoch 17 Iteration 6-- Accuracy 0.831 Sensitivity 0.850 Specificity 0.810 Area Under ROC 0.910 Val Loss 0.5363085120916367 Training Performance Epoch 17: Average loss: 0.0003, Accuracy: 424/425 (100%) Epoch 18 Iteration 6-- Accuracy 0.831 Sensitivity 0.833 Specificity 0.828 Area Under ROC 0.912 Val Loss 0.5382680594921112 Training Performance Epoch 18: Average loss: 0.0002, Accuracy: 425/425 (100%) Epoch 19 Iteration 6-- Accuracy 0.831 Sensitivity 0.833 Specificity 0.828 Area Under ROC 0.911 Val Loss 0.5439216196537018 Training Performance Epoch 19: Average loss: 0.0001, Accuracy: 424/425 (100%) Epoch 20 Iteration 6-- Accuracy 0.831 Sensitivity 0.867 Specificity 0.793 Area Under ROC 0.916 Val Loss 0.5402421057224274 Training Performance Epoch 20: Average loss: 0.0002, Accuracy: 424/425 (100%) Epoch 21 Iteration 6-- Accuracy 0.780 Sensitivity 0.717 Specificity 0.845 Area Under ROC 0.908 Val Loss 0.6748350858688354 Training Performance Epoch 21: Average loss: 0.0003, Accuracy: 424/425 (100%) Epoch 22 Iteration 6-- Accuracy 0.754 Sensitivity 0.617 Specificity 0.897 Area Under ROC 0.894 Val Loss 1.0918908566236496 Training Performance Epoch 22: Average loss: 0.0001, Accuracy: 425/425 (100%) Epoch 23 Iteration 6-- Accuracy 0.797 Sensitivity 0.750 Specificity 0.845 Area Under ROC 0.909 Val Loss 0.6290175318717957 Training Performance Epoch 23: Average loss: 0.0001, Accuracy: 423/425 (100%) . Test the model . Here we will load our trained model and calculate the metrics on the test data. . model = torch.load(&quot;best_model.pkl&quot; ) metrics_dict = calc_metrics(model, test_loader, plot_roc_curve = True) print(&#39;- Test Performance --&#39;) print(&quot;Accuracy t {:.3f}&quot;.format(metrics_dict[&#39;Accuracy&#39;])) print(&quot;Sensitivity t {:.3f}&quot;.format(metrics_dict[&#39;Sensitivity&#39;])) print(&quot;Specificity t {:.3f}&quot;.format(metrics_dict[&#39;Specificity&#39;])) print(&quot;Area Under ROC t {:.3f}&quot;.format(metrics_dict[&#39;Roc_score&#39;])) . - Test Performance -- Accuracy 0.749 Sensitivity 0.745 Specificity 0.752 Area Under ROC 0.845 . conf_matrix = metrics_dict[&quot;Confusion Matrix&quot;] ax= plt.subplot() sns.heatmap(conf_matrix, annot=True, ax = ax, cmap = &#39;Blues&#39;); #annot=True to annotate cells # labels, title and ticks ax.set_xlabel(&#39;Predicted labels&#39;);ax.set_ylabel(&#39;True labels&#39;); ax.set_title(&#39;Confusion Matrix&#39;); ax.xaxis.set_ticklabels([&#39;CoViD&#39;, &#39;NonCoViD&#39;]); ax.yaxis.set_ticklabels([&#39;CoViD&#39;, &#39;NonCoViD&#39;]) . [Text(0, 0.5, &#39;CoViD&#39;), Text(0, 1.5, &#39;NonCoViD&#39;)] . Recommendations : Scope for Improvements . Need More Data : At only 746 examples, the dataset is too small for a model trained on it to be deployed in the real world. . | Get Differentiated Data : As have been observed earlier, the data for the negative class of the data set contains patients suffering from other respiratory ailments. Very few among them will be completely healthy patients. The performance would have been much better if we had labels separating the healthy patients from the non-healthy non-COVID ones. This is perhaps the reason why our model mistakes non-COVID opacities for COVID ones. . | Use Better Networks : While we used a batch normalized VGG-19 pre-trained model, one can use advance architectures to learn the classifier. While transfer learning is a very successful technique, performing medical image application on models pretrained on ImageNet, a dataset of everyday items, may not be optimal. . | References . The below references have been extensively used to build this code set: . https://github.com/UCSD-AI4H/COVID-CT | https://blog.paperspace.com/fighting-coronavirus-with-ai-building-covid-19-classifier/ | https://github.com/heysachin/Malaria-detection-pytorch | https://arxiv.org/pdf/2003.13865.pdf | https://raw.githubusercontent.com/UCSD-AI4H/COVID-CT/master/baseline%20methods/DenseNet169/DenseNet_predict.py |",
            "url": "https://saptarshidatta.in/2020/11/03/COVID_19_Detection.html",
            "relUrl": "/2020/11/03/COVID_19_Detection.html",
            "date": " • Nov 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Implementation of YOLO v3 Algorithm",
            "content": "I expect everyone to have concepts regarding : . Non Maximum Suppression | Convolutional Neural Networks | PyTorch | OpenCV | Anchor Box | Bounding Box | YOLO v3 paper ==&gt; Check here! . I have followed the awesome explanation by Ayoosh Kathuria on the paperspace blog with some modifications in the code. . You will need to download the following files : . Official Configuration File . | Pallete File . | So let&#39;s dive in to the code. We start by importing the necessary libraries. . from __future__ import division import torch import torch.nn as nn import torch.nn.functional as F from torch.autograd import Variable import numpy as np import cv2 import time import os import os.path as osp import pickle as pkl import random import pandas as pd . Here we have written some utility functions. . def unique(tensor): tensor_np = tensor.cpu().numpy() unique_np = np.unique(tensor_np) unique_tensor = torch.from_numpy(unique_np) tensor_res = tensor.new(unique_tensor.shape) tensor_res.copy_(unique_tensor) return tensor_res def bbox_iou(box1, box2): &quot;&quot;&quot; Returns the IoU of two bounding boxes &quot;&quot;&quot; #Get the coordinates of bounding boxes b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3] b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3] #get the corrdinates of the intersection rectangle inter_rect_x1 = torch.max(b1_x1, b2_x1) inter_rect_y1 = torch.max(b1_y1, b2_y1) inter_rect_x2 = torch.min(b1_x2, b2_x2) inter_rect_y2 = torch.min(b1_y2, b2_y2) #Intersection area inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0) #Union Area b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1) b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1) iou = inter_area / (b1_area + b2_area - inter_area) return iou def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True): batch_size = prediction.size(0) print(prediction.size) stride = inp_dim // prediction.size(2) grid_size = inp_dim // stride bbox_attrs = 5 + num_classes num_anchors = len(anchors) prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size) prediction = prediction.transpose(1,2).contiguous() prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs) anchors = [(a[0]/stride, a[1]/stride) for a in anchors] #Sigmoid the centre_X, centre_Y. and object confidencce prediction[:,:,0] = torch.sigmoid(prediction[:,:,0]) prediction[:,:,1] = torch.sigmoid(prediction[:,:,1]) prediction[:,:,4] = torch.sigmoid(prediction[:,:,4]) #Add the center offsets grid = np.arange(grid_size) a,b = np.meshgrid(grid, grid) x_offset = torch.FloatTensor(a).view(-1,1) y_offset = torch.FloatTensor(b).view(-1,1) if CUDA: x_offset = x_offset.cuda() y_offset = y_offset.cuda() x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0) prediction[:,:,:2] += x_y_offset #log space transform height and the width anchors = torch.FloatTensor(anchors) if CUDA: anchors = anchors.cuda() anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0) prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes])) prediction[:,:,:4] *= stride return prediction def write_results(prediction, confidence, num_classes, nms_conf): conf_mask = (prediction[:,:,4] &gt; confidence).float().unsqueeze(2) prediction = prediction*conf_mask box_corner = prediction.new(prediction.shape) box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2) box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2) box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2) prediction[:,:,:4] = box_corner[:,:,:4] batch_size = prediction.size(0) write = False for ind in range(batch_size): image_pred = prediction[ind] #image Tensor #confidence thresholding #NMS max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1) max_conf = max_conf.float().unsqueeze(1) max_conf_score = max_conf_score.float().unsqueeze(1) seq = (image_pred[:,:5], max_conf, max_conf_score) image_pred = torch.cat(seq, 1) non_zero_ind = (torch.nonzero(image_pred[:,4])) try: image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7) except: continue if image_pred_.shape[0] == 0: continue #Get the various classes detected in the image img_classes = unique(image_pred_[:,-1]) # -1 index holds the class index for cls in img_classes: #perform NMS #get the detections with one particular class cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1) class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze() image_pred_class = image_pred_[class_mask_ind].view(-1,7) #sort the detections such that the entry with the maximum objectness #confidence is at the top conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1] image_pred_class = image_pred_class[conf_sort_index] idx = image_pred_class.size(0) #Number of detections for i in range(idx): #Get the IOUs of all boxes that come after the one we are looking at #in the loop try: ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:]) except ValueError: break except IndexError: break #Zero out all the detections that have IoU &gt; treshhold iou_mask = (ious &lt; nms_conf).float().unsqueeze(1) image_pred_class[i+1:] *= iou_mask #Remove the non-zero entries non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze() image_pred_class = image_pred_class[non_zero_ind].view(-1,7) batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind) #Repeat the batch_id for as many detections of the class cls in the image seq = batch_ind, image_pred_class if not write: output = torch.cat(seq,1) write = True else: out = torch.cat(seq,1) output = torch.cat((output,out)) try: return output except: return 0 def letterbox_image(img, inp_dim): &#39;&#39;&#39;resize image with unchanged aspect ratio using padding&#39;&#39;&#39; img_w, img_h = img.shape[1], img.shape[0] w, h = inp_dim new_w = int(img_w * min(w/img_w, h/img_h)) new_h = int(img_h * min(w/img_w, h/img_h)) resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC) canvas = np.full((inp_dim[1], inp_dim[0], 3), fill_value = 128) canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w, :] = resized_image return canvas def prep_image(img, inp_dim): &quot;&quot;&quot; Prepare image for inputting to the neural network. Converts Numpy Array to PyTorch&#39;s input format &quot;&quot;&quot; img = (letterbox_image(img, (inp_dim, inp_dim))) img = img[:,:,::-1].transpose((2,0,1)).copy() img = torch.from_numpy(img).float().div(255.0).unsqueeze(0) return img def load_classes(namesfile): fp = open(namesfile, &quot;r&quot;) names = fp.read().split(&quot; n&quot;)[:-1] return names . Now, we construct the network. We have a cfg file to help u create the network. . #img = cv2.imread(&quot;dog-cycle-car.png&quot;) #img = cv2.resize(img, (416,416)) #Resize to the input dimension #img_ = img[:,:,::-1].transpose((2,0,1)) # BGR -&gt; RGB | H X W C -&gt; C X H X W #img_ = img_[np.newaxis,:,:,:]/255.0 #Add a channel at 0 (for batch) | Normalise #img_ = torch.from_numpy(img_).float() #Convert to float #img_ = Variable(img_) # Convert to Variable #return img_ def parse_cfg(cfgfile): &quot;&quot;&quot; Takes a configuration file Returns a list of blocks. Each blocks describes a block in the neural network to be built. Block is represented as a dictionary in the list &quot;&quot;&quot; file = open(cfgfile, &#39;r&#39;) lines = file.read().split(&#39; n&#39;) # store the lines in a list lines = [x for x in lines if len(x) &gt; 0] # get read of the empty lines lines = [x for x in lines if x[0] != &#39;#&#39;] # get rid of comments lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces block = {} blocks = [] for line in lines: if line[0] == &quot;[&quot;: # This marks the start of a new block if len(block) != 0: # If block is not empty, implies it is storing values of previous block. blocks.append(block) # add it the blocks list block = {} # re-init the block block[&quot;type&quot;] = line[1:-1].rstrip() else: key,value = line.split(&quot;=&quot;) block[key.rstrip()] = value.lstrip() blocks.append(block) return blocks class EmptyLayer(nn.Module): def __init__(self): super(EmptyLayer, self).__init__() class DetectionLayer(nn.Module): def __init__(self, anchors): super(DetectionLayer, self).__init__() self.anchors = anchors def create_modules(blocks): net_info = blocks[0] #Captures the information about the input and pre-processing module_list = nn.ModuleList() prev_filters = 3 output_filters = [] for index, x in enumerate(blocks[1:]): module = nn.Sequential() #check the type of block #create a new module for the block #append to module_list #If it&#39;s a convolutional layer if (x[&quot;type&quot;] == &quot;convolutional&quot;): #Get the info about the layer activation = x[&quot;activation&quot;] try: batch_normalize = int(x[&quot;batch_normalize&quot;]) bias = False except: batch_normalize = 0 bias = True filters= int(x[&quot;filters&quot;]) padding = int(x[&quot;pad&quot;]) kernel_size = int(x[&quot;size&quot;]) stride = int(x[&quot;stride&quot;]) if padding: pad = (kernel_size - 1) // 2 else: pad = 0 #Add the convolutional layer conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias) module.add_module(&quot;conv_{0}&quot;.format(index), conv) #Add the Batch Norm Layer if batch_normalize: bn = nn.BatchNorm2d(filters) module.add_module(&quot;batch_norm_{0}&quot;.format(index), bn) #Check the activation. #It is either Linear or a Leaky ReLU for YOLO if activation == &quot;leaky&quot;: activn = nn.LeakyReLU(0.1, inplace = True) module.add_module(&quot;leaky_{0}&quot;.format(index), activn) #If it&#39;s an upsampling layer #We use Bilinear2dUpsampling elif (x[&quot;type&quot;] == &quot;upsample&quot;): stride = int(x[&quot;stride&quot;]) upsample = nn.Upsample(scale_factor = 2, mode = &quot;nearest&quot;) module.add_module(&quot;upsample_{}&quot;.format(index), upsample) #If it is a route layer elif (x[&quot;type&quot;] == &quot;route&quot;): x[&quot;layers&quot;] = x[&quot;layers&quot;].split(&#39;,&#39;) #Start of a route start = int(x[&quot;layers&quot;][0]) #end, if there exists one. try: end = int(x[&quot;layers&quot;][1]) except: end = 0 #Positive anotation if start &gt; 0: start = start - index if end &gt; 0: end = end - index route = EmptyLayer() module.add_module(&quot;route_{0}&quot;.format(index), route) if end &lt; 0: filters = output_filters[index + start] + output_filters[index + end] else: filters= output_filters[index + start] #shortcut corresponds to skip connection elif x[&quot;type&quot;] == &quot;shortcut&quot;: shortcut = EmptyLayer() module.add_module(&quot;shortcut_{}&quot;.format(index), shortcut) #Yolo is the detection layer elif x[&quot;type&quot;] == &quot;yolo&quot;: mask = x[&quot;mask&quot;].split(&quot;,&quot;) mask = [int(x) for x in mask] anchors = x[&quot;anchors&quot;].split(&quot;,&quot;) anchors = [int(a) for a in anchors] anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)] anchors = [anchors[i] for i in mask] detection = DetectionLayer(anchors) module.add_module(&quot;Detection_{}&quot;.format(index), detection) module_list.append(module) prev_filters = filters output_filters.append(filters) return (net_info, module_list) class Darknet(nn.Module): def __init__(self, cfgfile): super(Darknet, self).__init__() self.blocks = parse_cfg(cfgfile) self.net_info, self.module_list = create_modules(self.blocks) def forward(self, x, CUDA): modules = self.blocks[1:] outputs = {} #We cache the outputs for the route layer write = 0 for i, module in enumerate(modules): module_type = (module[&quot;type&quot;]) if module_type == &quot;convolutional&quot; or module_type == &quot;upsample&quot;: x = self.module_list[i](x) elif module_type == &quot;route&quot;: layers = module[&quot;layers&quot;] layers = [int(a) for a in layers] if (layers[0]) &gt; 0: layers[0] = layers[0] - i if len(layers) == 1: x = outputs[i + (layers[0])] else: if (layers[1]) &gt; 0: layers[1] = layers[1] - i map1 = outputs[i + layers[0]] map2 = outputs[i + layers[1]] x = torch.cat((map1, map2), 1) elif module_type == &quot;shortcut&quot;: from_ = int(module[&quot;from&quot;]) x = outputs[i-1] + outputs[i+from_] elif module_type == &#39;yolo&#39;: anchors = self.module_list[i][0].anchors print(&quot;Detection Layer =&gt; &quot; ,i) print(&quot;Anchors selected =&gt; &quot; ,anchors) #Get the input dimensions inp_dim = int (self.net_info[&quot;height&quot;]) #Get the number of classes num_classes = int (module[&quot;classes&quot;]) #Transform x = x.data print(&quot;Size before transform =&gt; &quot; ,x.size()) x = predict_transform(x, inp_dim, anchors, num_classes, CUDA) print(&quot;Size after transform =&gt; &quot; ,x.size()) if not write: #if no collector has been intialised. detections = x write = 1 else: detections = torch.cat((detections, x), 1) outputs[i] = x return detections def load_weights(self, weightfile): #Open the weights file fp = open(weightfile, &quot;rb&quot;) #The first 5 values are header information # 1. Major version number # 2. Minor Version Number # 3. Subversion number # 4,5. Images seen by the network (during training) header = np.fromfile(fp, dtype = np.int32, count = 5) self.header = torch.from_numpy(header) self.seen = self.header[3] weights = np.fromfile(fp, dtype = np.float32) ptr = 0 for i in range(len(self.module_list)): module_type = self.blocks[i + 1][&quot;type&quot;] #If module_type is convolutional load weights #Otherwise ignore. if module_type == &quot;convolutional&quot;: model = self.module_list[i] try: batch_normalize = int(self.blocks[i+1][&quot;batch_normalize&quot;]) except: batch_normalize = 0 conv = model[0] if (batch_normalize): bn = model[1] #Get the number of weights of Batch Norm Layer num_bn_biases = bn.bias.numel() #Load the weights bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases]) ptr += num_bn_biases bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases]) ptr += num_bn_biases bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases]) ptr += num_bn_biases bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases]) ptr += num_bn_biases #Cast the loaded weights into dims of model weights. bn_biases = bn_biases.view_as(bn.bias.data) bn_weights = bn_weights.view_as(bn.weight.data) bn_running_mean = bn_running_mean.view_as(bn.running_mean) bn_running_var = bn_running_var.view_as(bn.running_var) #Copy the data to model bn.bias.data.copy_(bn_biases) bn.weight.data.copy_(bn_weights) bn.running_mean.copy_(bn_running_mean) bn.running_var.copy_(bn_running_var) else: #Number of biases num_biases = conv.bias.numel() #Load the weights conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases]) ptr = ptr + num_biases #reshape the loaded weights according to the dims of the model weights conv_biases = conv_biases.view_as(conv.bias.data) #Finally copy the data conv.bias.data.copy_(conv_biases) #Let us load the weights for the Convolutional layers num_weights = conv.weight.numel() #Do the same as above for weights conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights]) ptr = ptr + num_weights conv_weights = conv_weights.view_as(conv.weight.data) conv.weight.data.copy_(conv_weights) . We will load the pretrained weights &amp; the class names of the COCO dataset. After downloading the files, you should comment the below two lines of code. . !wget https://pjreddie.com/media/files/yolov3.weights . !wget https://github.com/pjreddie/darknet/blob/master/data/coco.names . Now we load a single image or batches of image, and change the batch size accordingly. We also set the confidence score and the NMS Threshold here. Next, we will load the class names using the load_classes function. . We have set up the path =&gt; /content/drive/My Drive/test-img in Google Drive for importing inbound test images and path =&gt; /content/drive/My Drive/results to get the outbound test images. When running the code, you are expected to create file pathe of your own. . images = &#39;/content/drive/My Drive/test-img/&#39; batch_size = 1 confidence = 0.5 nms_thesh = 0.4 start = 0 CUDA = torch.cuda.is_available() classes = load_classes(&#39;/content/coco.names&#39;) . Here we will load the model with the cfg file and the weights file. . print(&quot;Loading network.....&quot;) model = Darknet(&#39;yolov3.cfg&#39;) model.load_weights(&#39;yolov3.weights&#39;) print(&quot;Network successfully loaded&quot;) #model.net_info[&quot;height&quot;] = 416 inp_dim = int(model.net_info[&quot;height&quot;]) assert inp_dim % 32 == 0 assert inp_dim &gt; 32 . Loading network..... Network successfully loaded . We will now evaluate the model. . model.eval() read_dir = time.time() #Detection phase try: imlist = [osp.join(osp.realpath(&#39;.&#39;), images, img) for img in os.listdir(images)] except NotADirectoryError: imlist = [] imlist.append(osp.join(osp.realpath(&#39;.&#39;), images)) except FileNotFoundError: print (&quot;No file or directory with the name {}&quot;.format(images)) exit() if not os.path.exists(&#39;/content/drive/My Drive/results&#39;): os.makedirs(&#39;/content/drive/My Drive/results&#39;) load_batch = time.time() loaded_ims = [cv2.imread(x) for x in imlist] im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))])) im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims] im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2) leftover = 0 if (len(im_dim_list) % batch_size): leftover = 1 if batch_size != 1: num_batches = len(imlist) // batch_size + leftover im_batches = [torch.cat((im_batches[i*batch_size : min((i + 1)*batch_size, len(im_batches))])) for i in range(num_batches)] write = 0 if CUDA: im_dim_list = im_dim_list.cuda() start_det_loop = time.time() for i, batch in enumerate(im_batches): #load the image start = time.time() if CUDA: batch = batch.cuda() with torch.no_grad(): prediction = model(Variable(batch), CUDA) prediction = write_results(prediction, confidence, num_classes = 80, nms_conf = nms_thesh) end = time.time() if type(prediction) == int: for im_num, image in enumerate(imlist[i*batch_size: min((i + 1)*batch_size, len(imlist))]): im_id = i*batch_size + im_num print(&quot;{0:20s} predicted in {1:6.3f} seconds&quot;.format(image.split(&quot;/&quot;)[-1], (end - start)/batch_size)) print(&quot;{0:20s} {1:s}&quot;.format(&quot;Objects Detected:&quot;, &quot;&quot;)) print(&quot;-&quot;) continue prediction[:,0] += i*batch_size #transform the atribute from index in batch to index in imlist if not write: #If we have&#39;t initialised output output = prediction write = 1 else: output = torch.cat((output,prediction)) for im_num, image in enumerate(imlist[i*batch_size: min((i + 1)*batch_size, len(imlist))]): im_id = i*batch_size + im_num objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id] print(&quot;{0:20s} predicted in {1:6.3f} seconds&quot;.format(image.split(&quot;/&quot;)[-1], (end - start)/batch_size)) print(&quot;{0:20s} {1:s}&quot;.format(&quot;Objects Detected:&quot;, &quot; &quot;.join(objs))) print(&quot;-&quot;) if CUDA: torch.cuda.synchronize() try: output except NameError: print (&quot;No detections were made&quot;) exit() im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long()) scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1) output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2 output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2 output[:,1:5] /= scaling_factor for i in range(output.shape[0]): output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0]) output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1]) output_recast = time.time() class_load = time.time() colors = pkl.load(open(&quot;pallete&quot;, &quot;rb&quot;)) draw = time.time() def write(x, results): c1 = tuple(x[1:3].int()) c2 = tuple(x[3:5].int()) img = results[int(x[0])] cls = int(x[-1]) color = random.choice(colors) label = &quot;{0}&quot;.format(classes[cls]) cv2.rectangle(img, c1, c2, color, 6) t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0] c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4 cv2.rectangle(img, c1, c2, color, 6) cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1); return img list(map(lambda x: write(x, loaded_ims), output)) det_names = pd.Series(imlist).apply(lambda x: &quot;{}/det_{}&quot;.format(&#39;/content/drive/My Drive/results&#39;,x.split(&quot;/&quot;)[-1])) list(map(cv2.imwrite, det_names, loaded_ims)) end = time.time() print(&quot;SUMMARY&quot;) print(&quot;-&quot;) print(&quot;{:25s}: {}&quot;.format(&quot;Task&quot;, &quot;Time Taken (in seconds)&quot;)) print() print(&quot;{:25s}: {:2.3f}&quot;.format(&quot;Reading addresses&quot;, load_batch - read_dir)) print(&quot;{:25s}: {:2.3f}&quot;.format(&quot;Loading batch&quot;, start_det_loop - load_batch)) print(&quot;{:25s}: {:2.3f}&quot;.format(&quot;Detection (&quot; + str(len(imlist)) + &quot; images)&quot;, output_recast - start_det_loop)) print(&quot;{:25s}: {:2.3f}&quot;.format(&quot;Output Processing&quot;, class_load - output_recast)) print(&quot;{:25s}: {:2.3f}&quot;.format(&quot;Drawing Boxes&quot;, end - draw)) print(&quot;{:25s}: {:2.3f}&quot;.format(&quot;Average time_per_img&quot;, (end - load_batch)/len(imlist))) print(&quot;-&quot;) torch.cuda.empty_cache() . Detection Layer =&gt; 82 Anchors selected =&gt; [(116, 90), (156, 198), (373, 326)] Size before transform =&gt; torch.Size([1, 255, 13, 13]) &lt;built-in method size of Tensor object at 0x7f900f935630&gt; Size after transform =&gt; torch.Size([1, 507, 85]) Detection Layer =&gt; 94 Anchors selected =&gt; [(30, 61), (62, 45), (59, 119)] Size before transform =&gt; torch.Size([1, 255, 26, 26]) &lt;built-in method size of Tensor object at 0x7f900f9424c8&gt; Size after transform =&gt; torch.Size([1, 2028, 85]) Detection Layer =&gt; 106 Anchors selected =&gt; [(10, 13), (16, 30), (33, 23)] Size before transform =&gt; torch.Size([1, 255, 52, 52]) &lt;built-in method size of Tensor object at 0x7f900f942828&gt; Size after transform =&gt; torch.Size([1, 8112, 85]) cricket.jpg predicted in 1.275 seconds Objects Detected: &lt;script crossorigin=&#34;anonymous&#34; defer=&#34;defer&#34; integrity=&#34;sha512-JDDF8W8Wl5vopo9t4K4NtIEUMCYov3ZjVpv9lC1SDUxhejU+ILu8V3l6BhkaIRMYJioQWj9am9tJSTvND+8wJg==&#34; type=&#34;application/javascript&#34; data-module-id=&#34;./chunk-drag-drop.js&#34; data-src=&#34;https://github.githubassets.com/assets/chunk-drag-drop-2430c5f1.js&#34;&gt;&lt;/script&gt; - SUMMARY - Task : Time Taken (in seconds) Reading addresses : 0.001 Loading batch : 0.018 Detection (1 images) : 1.278 Output Processing : 0.000 Drawing Boxes : 0.020 Average time_per_img : 1.316 - . I tested my network with a image I clicked during Durga Pujo 2020! . Detected Image: . .",
            "url": "https://saptarshidatta.in/2020/10/26/YOLOv3_Implementation.html",
            "relUrl": "/2020/10/26/YOLOv3_Implementation.html",
            "date": " • Oct 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "CNN Training Loop Refactoring (Simultaneous Hyperparameter Testing)",
            "content": "When we last trained our network, we built out quite a lot of functionality that allowed us to experiment with many different parameters and values, and we also made the calls need inside our training loop that would get our results into TensorBoard. . All of this work has helped, but our training loop is quite crowded now. In this exercise, we&#39;re going to clean up our training loop and set the stage for more experimentation up by using the RunBuilder class that we built last time and by building a new class called RunManager. . I also find this way of Hyperparameter Tuning more intuitive than TensorBoard. Also, as our number of parameters and runs get larger, TensorBoard will start to breakdown as a viable solution for reviewing our results. . However, calls have been made inside our RunManager class to TensorBoard, so it can be used as an added functionality. For reference, on how to use TensorBoard with PyTorch inside Google Collab, plese refer here. . The code also generates results in csv and json format, which can be used gor further analysis. . import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter from IPython.display import display, clear_output import pandas as pd import time import json from itertools import product from collections import namedtuple from collections import OrderedDict . Designing the Neural Network . class Network(nn.Module): def __init__(self): super(Network,self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.fc1 = nn.Linear(in_features=16*5*5, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=84) self.out = nn.Linear(in_features=84, out_features=10) def forward(self, t): #Layer 1 t = t #Layer 2 t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (6,14,14) #Layer 3 t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (16,5,5) #Layer 4 t = t.reshape(-1, 16*5*5) t = self.fc1(t) t = F.relu(t)#output shape : (1,120) #Layer 5 t = self.fc2(t) t = F.relu(t)#output shape : (1, 84) #Layer 6/ Output Layer t = self.out(t)#output shape : (1,10) return t . RunBuilder class . class RunBuilder(): @staticmethod def get_runs(params): Run = namedtuple(&#39;Run&#39;, params.keys()) runs = [] for v in product(*params.values()): runs.append(Run(*v)) return runs . RunManager class . class RunManager(): def __init__(self): self.epoch_count = 0 self.epoch_loss = 0 self.epoch_num_correct = 0 self.epoch_start_time = None self.run_params = None self.run_count = 0 self.run_data = [] self.run_start_time = None self.network = None self.loader = None self.tb = None def begin_run(self, run, network, loader): self.run_start_time = time.time() self.run_params = run self.run_count += 1 self.network = network self.loader = loader self.tb = SummaryWriter(comment=f&#39;-{run}&#39;) images, labels = next(iter(self.loader)) grid = torchvision.utils.make_grid(images) self.tb.add_image(&#39;images&#39;, grid) self.tb.add_graph( self.network ,images.to(getattr(run, &#39;device&#39;, &#39;cpu&#39;)) ) def end_run(self): self.tb.close() self.epoch_count = 0 def begin_epoch(self): self.epoch_start_time = time.time() self.epoch_count += 1 self.epoch_loss = 0 self.epoch_num_correct = 0 def end_epoch(self): epoch_duration = time.time() - self.epoch_start_time run_duration = time.time() - self.run_start_time loss = self.epoch_loss / len(self.loader.dataset) accuracy = self.epoch_num_correct / len(self.loader.dataset) self.tb.add_scalar(&#39;Loss&#39;, loss, self.epoch_count) self.tb.add_scalar(&#39;Accuracy&#39;, accuracy, self.epoch_count) for name, param in self.network.named_parameters(): self.tb.add_histogram(name, param, self.epoch_count) self.tb.add_histogram(f&#39;{name}.grad&#39;, param.grad, self.epoch_count) results = OrderedDict() results[&quot;run&quot;] = self.run_count results[&quot;epoch&quot;] = self.epoch_count results[&#39;loss&#39;] = loss results[&quot;accuracy&quot;] = accuracy results[&#39;epoch duration&#39;] = epoch_duration results[&#39;run duration&#39;] = run_duration for k,v in self.run_params._asdict().items(): results[k] = v self.run_data.append(results) df = pd.DataFrame.from_dict(self.run_data, orient=&#39;columns&#39;) clear_output(wait=True) display(df) def track_loss(self, loss, batch): self.epoch_loss += loss.item() * batch[0].shape[0] def track_num_correct(self, preds, labels): self.epoch_num_correct += self._get_num_correct(preds, labels) def _get_num_correct(self, preds, labels): return preds.argmax(dim=1).eq(labels).sum().item() def save(self, fileName): pd.DataFrame.from_dict( self.run_data ,orient=&#39;columns&#39; ).to_csv(f&#39;{fileName}.csv&#39;) with open(f&#39;{fileName}.json&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: json.dump(self.run_data, f, ensure_ascii=False, indent=4) . Loading the CIFAR-10 data and pre-processing . transform =transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_set = torchvision.datasets.CIFAR10( root=&#39;./data&#39; ,train=True ,download=True ,transform=transform ) . Files already downloaded and verified . Training the Nueral Network . params = OrderedDict( lr = [.01] ,batch_size = [1000] ,shuffle = [True] ,num_workers = [0, 1, 2, 4, 8, 16] ) m = RunManager() for run in RunBuilder.get_runs(params): network = Network() loader = DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle, num_workers=run.num_workers) optimizer = optim.Adam(network.parameters(), lr=run.lr) m.begin_run(run, network, loader) for epoch in range(1): m.begin_epoch() for batch in loader: images, labels = batch preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # Calculate Loss optimizer.zero_grad() # Zero Gradients loss.backward() # Calculate Gradients optimizer.step() # Update Weights m.track_loss(loss, batch) m.track_num_correct(preds, labels) m.end_epoch() m.end_run() m.save(&#39;results&#39;) . run epoch loss accuracy epoch duration run duration lr batch_size shuffle num_workers . 0 1 | 1 | 1.869639 | 0.30556 | 18.566388 | 20.198124 | 0.01 | 1000 | True | 0 | . 1 2 | 1 | 1.967650 | 0.26044 | 16.305351 | 18.226968 | 0.01 | 1000 | True | 1 | . 2 3 | 1 | 1.930892 | 0.27954 | 15.927075 | 18.112834 | 0.01 | 1000 | True | 2 | . 3 4 | 1 | 1.850479 | 0.30830 | 16.800490 | 19.740744 | 0.01 | 1000 | True | 4 | . 4 5 | 1 | 1.822490 | 0.32622 | 17.848449 | 21.122425 | 0.01 | 1000 | True | 8 | . 5 6 | 1 | 1.865499 | 0.30920 | 19.867091 | 24.908000 | 0.01 | 1000 | True | 16 | . Experimenting with DataLoader num_workers attribute . The num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, and a value of zero tells the loader to load the data inside the main process. . This means that the training process will work sequentially inside the main process. After a batch is used during the training process and another one is needed, we read the batch data from disk. . Now, if we have a worker process, we can make use of the facility that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. The batches are loaded using additional worker processes and are queued up in memory. . The main take-away from these results is that having a single worker process in addition to the main process resulted in a speed up of about twenty percent. However, adding additional worker processes after the first one didn&#39;t really show any further improvements. . Additionally, we can see with higher number of num_workers results in higher run times. Please go through this link to know more. . Summary . We have introduced a way to experiment with Hyperparameters to extract maximum efficiency for our model. This code can be scaled up or scaled down to change the Hyperparameters we wish to experiment upon. . This may be noted that, accuracy is not that high as we have trained our model for 1 epoch with each set of parameters. This has been purely done for experimentation purpose. . However, we might need to change our network architecture i.e. a deeper network for higher efficiency. .",
            "url": "https://saptarshidatta.in/2020/10/11/PyTorch_CIFAR_10_Refactoring.html",
            "relUrl": "/2020/10/11/PyTorch_CIFAR_10_Refactoring.html",
            "date": " • Oct 11, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Neural Network and Deep Learning with PyTorch (TensorBoard and Hyperparameter Tuning)",
            "content": "In the previous post, we achieved an accuracy around 61%. In this exercise, we will try increasing the accuracy by Hyperparameter Tuning with tensorBoard. . For this exercise, we will use the CIFAR10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size. . . The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. . pip install tensorboardX . Collecting tensorboardX Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB) |████████████████████████████████| 317kB 2.7MB/s Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5) Requirement already satisfied: protobuf&gt;=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.8.0-&gt;tensorboardX) (50.3.0) Installing collected packages: tensorboardX Successfully installed tensorboardX-2.1 . pip install tensorboard . Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (2.3.0) Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.10.0) Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.32.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.2.2) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.0.1) Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.7.0) Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (2.23.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.12.4) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (50.3.0) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.15.0) Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.4.1) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.35.1) Requirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.18.5) Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.17.2) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.6/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard) (2.0.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard) (2020.6.20) Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard) (1.3.0) Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard) (4.6) Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard) (0.2.8) Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard) (4.1.1) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;markdown&gt;=2.6.8-&gt;tensorboard) (3.2.0) Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard) (3.1.0) Requirement already satisfied: pyasn1&gt;=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa&lt;5,&gt;=3.1.4; python_version &gt;= &#34;3&#34;-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard) (0.4.8) . import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix from tensorboardX import SummaryWriter from itertools import product . print(torch.__version__) print(torchvision.__version__) . 1.6.0+cu101 0.7.0+cu101 . Loading the CIFAR-10 data and pre-processing . transform =transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_set = torchvision.datasets.CIFAR10(root = &#39;./data&#39;, train=True, transform=transform, download=True) train_loader = torch.utils.data.DataLoader(train_set, batch_size = 4, shuffle=True) test_set = torchvision.datasets.CIFAR10(root = &#39;./data&#39;, train=False, transform=transform, download=True) test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified . Neural Network and PyTorch design . def get_num_correct(preds, labels): return preds.argmax(dim=1).eq(labels).sum().item() . class Network(nn.Module): def __init__(self): super(Network,self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.fc1 = nn.Linear(in_features=16*5*5, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=84) self.out = nn.Linear(in_features=84, out_features=10) def forward(self, t): #Layer 1 t = t #Layer 2 t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (6,14,14) #Layer 3 t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (16,5,5) #Layer 4 t = t.reshape(-1, 16*5*5) t = self.fc1(t) t = F.relu(t)#output shape : (1,120) #Layer 5 t = self.fc2(t) t = F.relu(t)#output shape : (1, 84) #Layer 6/ Output Layer t = self.out(t)#output shape : (1,10) return t . Training the Neural Network . At first, we will experiment with various values of learning rate and batch size. We have also used the Shuffle value to True or false inthis particular case for demonstration. Shuffle for a training data should always be set to True. . parameters = dict( lr = [.01, .001] ,batch_size = [100, 1000] ,shuffle = [True, False] ) param_values = [v for v in parameters.values()] param_values . [[0.01, 0.001], [100, 1000], [True, False]] . for lr, batch_size, shuffle in product(*param_values): print (lr, batch_size, shuffle) . 0.01 100 True 0.01 100 False 0.01 1000 True 0.01 1000 False 0.001 100 True 0.001 100 False 0.001 1000 True 0.001 1000 False . We will train the network for each combination for 5 epochs. . for lr, batch_size, shuffle in product(*param_values): comment = f&#39; batch_size={batch_size} lr={lr} shuffle={shuffle}&#39; network = Network() train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle) optimizer = optim.Adam(network.parameters(), lr=lr) images, labels = next(iter(train_loader)) grid = torchvision.utils.make_grid(images) tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(network, images) for epoch in range(5): total_loss = 0 total_correct = 0 for batch in train_loader: images, labels = batch # Get Batch preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # Calculate Loss optimizer.zero_grad() # Zero Gradients loss.backward() # Calculate Gradients optimizer.step() # Update Weights total_loss += loss.item() * batch_size total_correct += get_num_correct(preds, labels) tb.add_scalar(&#39;Loss&#39;, total_loss, epoch) tb.add_scalar(&#39;Number Correct&#39;, total_correct, epoch) tb.add_scalar(&#39;Accuracy&#39;, total_correct / len(train_set), epoch) for name, param in network.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&#39;{name}.grad&#39;, param.grad, epoch) print(&quot;epoch&quot;, epoch, &quot;total_correct:&quot;, total_correct, &quot;loss:&quot;, total_loss) tb.close() . epoch 0 total_correct: 18913 loss: 84416.04677438736 epoch 1 total_correct: 22884 loss: 75011.52611970901 epoch 2 total_correct: 24004 loss: 72498.98253679276 epoch 3 total_correct: 24446 loss: 71079.53473329544 epoch 4 total_correct: 25172 loss: 69767.30787754059 epoch 0 total_correct: 18080 loss: 85815.07077217102 epoch 1 total_correct: 22393 loss: 75708.69245529175 epoch 2 total_correct: 23443 loss: 73317.35738515854 epoch 3 total_correct: 24138 loss: 71403.88944149017 epoch 4 total_correct: 24549 loss: 70284.72211360931 epoch 0 total_correct: 16158 loss: 91712.37576007843 epoch 1 total_correct: 23656 loss: 72345.58689594269 epoch 2 total_correct: 26402 loss: 65827.77202129364 epoch 3 total_correct: 27780 loss: 62125.81276893616 epoch 4 total_correct: 29018 loss: 58600.123167037964 epoch 0 total_correct: 13378 loss: 97205.75332641602 epoch 1 total_correct: 21074 loss: 78503.47900390625 epoch 2 total_correct: 24219 loss: 70818.36271286011 epoch 3 total_correct: 25917 loss: 66917.46950149536 epoch 4 total_correct: 27061 loss: 64042.30237007141 epoch 0 total_correct: 18627 loss: 85521.23121023178 epoch 1 total_correct: 24836 loss: 69433.90280008316 epoch 2 total_correct: 27425 loss: 63158.92490148544 epoch 3 total_correct: 29103 loss: 58870.00452876091 epoch 4 total_correct: 30088 loss: 55997.095000743866 epoch 0 total_correct: 18632 loss: 86631.54971599579 epoch 1 total_correct: 24611 loss: 70948.91862869263 epoch 2 total_correct: 27004 loss: 64610.34981012344 epoch 3 total_correct: 28478 loss: 60442.202669382095 epoch 4 total_correct: 29625 loss: 57259.26607251167 epoch 0 total_correct: 12380 loss: 102817.72804260254 epoch 1 total_correct: 18081 loss: 87027.38559246063 epoch 2 total_correct: 20270 loss: 81504.30190563202 epoch 3 total_correct: 21327 loss: 78629.82904911041 epoch 4 total_correct: 22309 loss: 76317.25633144379 epoch 0 total_correct: 12731 loss: 102762.18891143799 epoch 1 total_correct: 18111 loss: 87888.2976770401 epoch 2 total_correct: 20812 loss: 80129.80782985687 epoch 3 total_correct: 22642 loss: 75342.45681762695 epoch 4 total_correct: 24057 loss: 71779.08217906952 . %load_ext tensorboard %tensorboard --logdir runs . Hyperparameter Tuning . We get the highest accuracy with batch size = 100, learning rate = 0.001. . | We get the least loss with batch size = 100, learning rate = 0.001. . | We get the highest number of correct with batch size = 100, learning rate = 0.001 . | . Next, we will train our model with the above hyperparameters for 20 epochs. The Shuffle should always be set to True for a dataset that is to be used for training. for better results, try training with 30-40 epochs. . network = Network() optimizer = optim.Adam(network.parameters(), lr=0.001) train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100, shuffle=True) for epoch in range(20): total_correct = 0 total_loss = 0 for batch in train_loader: #Get batch images, labels = batch #Unpack the batch into images and labels preds = network(images) #Pass batch loss = F.cross_entropy(preds, labels) #Calculate Loss optimizer.zero_grad() loss.backward() #Calculate gradients optimizer.step() #Update weights total_loss += loss.item() * batch_size total_correct += preds.argmax(dim=1).eq(labels).sum().item() print(&#39;epoch:&#39;, epoch, &quot;total_correct:&quot;, total_correct, &quot;loss:&quot;, total_loss) print(&#39;&gt;&gt;&gt; Training Complete &gt;&gt;&gt;&#39;) . epoch: 0 total_correct: 18530 loss: 853069.2903995514 epoch: 1 total_correct: 25004 loss: 694644.5602178574 epoch: 2 total_correct: 27074 loss: 637538.0937457085 epoch: 3 total_correct: 28671 loss: 598220.732986927 epoch: 4 total_correct: 30016 loss: 566515.201985836 epoch: 5 total_correct: 30900 loss: 541954.4064998627 epoch: 6 total_correct: 31552 loss: 521362.08987236023 epoch: 7 total_correct: 32245 loss: 502520.00361680984 epoch: 8 total_correct: 32864 loss: 488691.7136311531 epoch: 9 total_correct: 33429 loss: 472790.4091477394 epoch: 10 total_correct: 33773 loss: 458680.98479509354 epoch: 11 total_correct: 34350 loss: 444449.317753315 epoch: 12 total_correct: 34740 loss: 432666.919529438 epoch: 13 total_correct: 35213 loss: 419870.6297278404 epoch: 14 total_correct: 35589 loss: 407635.0798010826 epoch: 15 total_correct: 36037 loss: 396796.40060663223 epoch: 16 total_correct: 36279 loss: 389014.71281051636 epoch: 17 total_correct: 36739 loss: 377336.72922849655 epoch: 18 total_correct: 37137 loss: 366468.3187007904 epoch: 19 total_correct: 37374 loss: 357310.61974167824 &gt;&gt;&gt; Training Complete &gt;&gt;&gt; . Predictions . We will define a function to get all predictions at once. . @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: images, labels = batch preds = model(images) all_preds = torch.cat((all_preds, preds) ,dim=0) return all_preds . test_preds = get_all_preds(network, test_loader) actual_labels = torch.Tensor(test_set.targets) preds_correct = test_preds.argmax(dim=1).eq(actual_labels).sum().item() print(&#39;total correct:&#39;, preds_correct) print(&#39;accuracy:&#39;, preds_correct / len(test_set)) . total correct: 6277 accuracy: 0.6277 . The model predicted the label with 63 % accuracy, which is NOT that better than what we predicted without hyper-parameter tuning. Next, we will develop a confusion matrix which will demonstrate, in which particular areas our model is performing poorly. . Confusion Matrix . import itertools import numpy as np import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . cm = confusion_matrix(test_set.targets, test_preds.argmax(dim=1)) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) plt.figure(figsize=(10,10)) plot_confusion_matrix(cm, classes) . Confusion matrix, without normalization [[763 28 33 22 12 14 10 12 58 48] [ 38 766 7 8 3 9 9 2 28 130] [100 9 528 65 57 122 43 43 16 17] [ 43 15 74 323 41 365 46 52 15 26] [ 45 11 110 66 437 103 82 114 17 15] [ 31 9 73 105 18 657 20 66 6 15] [ 14 17 71 72 36 81 663 19 12 15] [ 23 3 38 27 42 108 5 705 5 44] [133 44 11 20 2 14 3 11 711 51] [ 70 114 10 18 4 12 6 23 19 724]] .",
            "url": "https://saptarshidatta.in/2020/10/06/PyTorch_CIFAR10_TB.html",
            "relUrl": "/2020/10/06/PyTorch_CIFAR10_TB.html",
            "date": " • Oct 6, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Neural Network and Deep Learning with PyTorch (CIFAR-10)",
            "content": "For this exercise, we will use the CIFAR10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size. . . The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. . import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix from torch.utils.tensorboard import SummaryWriter . print(torch.__version__) print(torchvision.__version__) . 1.6.0+cu101 0.7.0+cu101 . Loading the CIFAR-10 data and pre-processing . transform =transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_set = torchvision.datasets.CIFAR10(root = &#39;./data&#39;, train=True, transform=transform, download=True) train_loader = torch.utils.data.DataLoader(train_set, batch_size = 4, shuffle=True) test_set = torchvision.datasets.CIFAR10(root = &#39;./data&#39;, train=False, transform=transform, download=True) test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified . Let&#39;s view some of the training images in the way they are passed as batches to the Neural Network during the training process. . batch = next(iter(train_loader)) images, labels = batch print(images.shape) print(labels.shape) grid = torchvision.utils.make_grid(images, nrow=10) plt.figure(figsize=(15,15)) plt.imshow(grid.permute(1,2,0)) print(&#39;labels:&#39;, labels) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . torch.Size([4, 3, 32, 32]) torch.Size([4]) labels: tensor([2, 7, 2, 3]) . . Neural Network and PyTorch design . class Network(nn.Module): def __init__(self): super(Network,self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.fc1 = nn.Linear(in_features=16*5*5, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=84) self.out = nn.Linear(in_features=84, out_features=10) def forward(self, t): #Layer 1 t = t #Layer 2 t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (6,14,14) #Layer 3 t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (16,5,5) #Layer 4 t = t.reshape(-1, 16*5*5) t = self.fc1(t) t = F.relu(t)#output shape : (1,120) #Layer 5 t = self.fc2(t) t = F.relu(t)#output shape : (1, 84) #Layer 6/ Output Layer t = self.out(t)#output shape : (1,10) return t network = Network() . Training the Neural Network . Defining a Loss Function and a Optimizer. For more details on Optimizer, please click here. . optimizer = optim.Adam(network.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) . for epoch in range(5): total_correct = 0 total_loss = 0 for batch in train_loader: #Get batch images, labels = batch #Unpack the batch into images and labels preds = network(images) #Pass batch loss = F.cross_entropy(preds, labels) #Calculate Loss optimizer.zero_grad() loss.backward() #Calculate gradients optimizer.step() #Update weights total_loss += loss.item() total_correct += preds.argmax(dim=1).eq(labels).sum().item() print(&#39;epoch:&#39;, epoch, &quot;total_correct:&quot;, total_correct, &quot;loss:&quot;, total_loss) print(&#39;&gt;&gt;&gt; Training Complete &gt;&gt;&gt;&#39;) . epoch: 0 total_correct: 22538 loss: 18830.44906747341 epoch: 1 total_correct: 28122 loss: 15425.85352185741 epoch: 2 total_correct: 29871 loss: 14222.835543203168 epoch: 3 total_correct: 31315 loss: 13312.894761506468 epoch: 4 total_correct: 32041 loss: 12740.08337423997 &gt;&gt;&gt; Training Complete &gt;&gt;&gt; . Let&#39;s quickly save the model, we just trained. . PATH = &#39;./cifar_net.pth&#39; torch.save(network.state_dict(), PATH) . Predictions . We will define a function to get all predictions at once. Now, let&#39;s load our Saved Model. Please note that saving and re-loading the model wasn’t necessary here, we only did it to illustrate how to do so. . network = Network() network.load_state_dict(torch.load(PATH)) . &lt;All keys matched successfully&gt; . @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: images, labels = batch preds = model(images) all_preds = torch.cat((all_preds, preds) ,dim=0) return all_preds . test_preds = get_all_preds(network, test_loader) actual_labels = torch.Tensor(test_set.targets) preds_correct = test_preds.argmax(dim=1).eq(actual_labels).sum().item() print(&#39;total correct:&#39;, preds_correct) print(&#39;accuracy:&#39;, preds_correct / len(test_set)) . total correct: 6083 accuracy: 0.6083 . The model predicted the label with 61 % accuracy, which is not that great. Next, we will develop a confusion matrix which will demonstrate, in which particular areas our model is performing poorly. . Confusion Matrix . import itertools import numpy as np import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . cm = confusion_matrix(test_set.targets, test_preds.argmax(dim=1)) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) plt.figure(figsize=(10,10)) plot_confusion_matrix(cm, classes) . Confusion matrix, without normalization [[656 29 52 42 45 14 7 18 77 60] [ 32 681 5 28 7 5 8 12 37 185] [ 72 11 366 134 181 120 50 36 13 17] [ 15 9 34 473 108 230 43 45 16 27] [ 21 7 25 110 630 70 42 81 9 5] [ 10 3 29 207 75 570 15 70 6 15] [ 4 11 39 94 122 48 653 12 4 13] [ 10 2 16 88 103 122 3 638 3 15] [112 46 13 30 17 16 5 10 667 84] [ 34 82 9 43 11 13 6 26 27 749]] .",
            "url": "https://saptarshidatta.in/2020/10/05/PyTorch_CIFAR10.html",
            "relUrl": "/2020/10/05/PyTorch_CIFAR10.html",
            "date": " • Oct 5, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Sign Language Prediction with MobileNet - Code",
            "content": "from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/gdrive . import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras.layers import Dense, Activation from tensorflow.keras.optimizers import Adam from tensorflow.keras.metrics import categorical_crossentropy from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.preprocessing import image from tensorflow.keras.models import Model from tensorflow.keras.applications import imagenet_utils from sklearn.metrics import confusion_matrix import itertools import os import shutil import random import matplotlib.pyplot as plt %matplotlib inline . Data Preparation . See the associated blog for more description regarding this. . os.chdir(&#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset&#39;) if os.path.isdir(&#39;train/0/&#39;) is False: os.mkdir(&#39;train&#39;) os.mkdir(&#39;valid&#39;) os.mkdir(&#39;test&#39;) for i in range(0, 10): shutil.move(f&#39;{i}&#39;, &#39;train&#39;) os.mkdir(f&#39;valid/{i}&#39;) os.mkdir(f&#39;test/{i}&#39;) valid_samples = random.sample(os.listdir(f&#39;train/{i}&#39;), 30) for j in valid_samples: shutil.move(f&#39;train/{i}/{j}&#39;, f&#39;valid/{i}&#39;) test_samples = random.sample(os.listdir(f&#39;train/{i}&#39;), 5) for k in test_samples: shutil.move(f&#39;train/{i}/{k}&#39;, f&#39;test/{i}&#39;) os.chdir(&#39;../..&#39;) . for i in range(0, 10): assert len(os.listdir(f&#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset/valid/{i}&#39;)) == 30 assert len(os.listdir(f&#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset/test/{i}&#39;)) == 5 . train_path = &#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset/train&#39; valid_path = &#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset/valid&#39; test_path = &#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset/test&#39; . train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory( directory=train_path, target_size=(224,224), batch_size=10) valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory( directory=valid_path, target_size=(224,224), batch_size=10) test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory( directory=test_path, target_size=(224,224), batch_size=10, shuffle=False) . Found 1712 images belonging to 10 classes. Found 300 images belonging to 10 classes. Found 50 images belonging to 10 classes. . Model Import, Transfer Learning and Training . Imporing Mobile Net . mobile = tf.keras.applications.mobilenet.MobileNet() . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5 17227776/17225924 [==============================] - 0s 0us/step . mobile.summary() . Model: &#34;mobilenet_1.00_224&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ conv1_pad (ZeroPadding2D) (None, 225, 225, 3) 0 _________________________________________________________________ conv1 (Conv2D) (None, 112, 112, 32) 864 _________________________________________________________________ conv1_bn (BatchNormalization (None, 112, 112, 32) 128 _________________________________________________________________ conv1_relu (ReLU) (None, 112, 112, 32) 0 _________________________________________________________________ conv_dw_1 (DepthwiseConv2D) (None, 112, 112, 32) 288 _________________________________________________________________ conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32) 128 _________________________________________________________________ conv_dw_1_relu (ReLU) (None, 112, 112, 32) 0 _________________________________________________________________ conv_pw_1 (Conv2D) (None, 112, 112, 64) 2048 _________________________________________________________________ conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64) 256 _________________________________________________________________ conv_pw_1_relu (ReLU) (None, 112, 112, 64) 0 _________________________________________________________________ conv_pad_2 (ZeroPadding2D) (None, 113, 113, 64) 0 _________________________________________________________________ conv_dw_2 (DepthwiseConv2D) (None, 56, 56, 64) 576 _________________________________________________________________ conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64) 256 _________________________________________________________________ conv_dw_2_relu (ReLU) (None, 56, 56, 64) 0 _________________________________________________________________ conv_pw_2 (Conv2D) (None, 56, 56, 128) 8192 _________________________________________________________________ conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128) 512 _________________________________________________________________ conv_pw_2_relu (ReLU) (None, 56, 56, 128) 0 _________________________________________________________________ conv_dw_3 (DepthwiseConv2D) (None, 56, 56, 128) 1152 _________________________________________________________________ conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128) 512 _________________________________________________________________ conv_dw_3_relu (ReLU) (None, 56, 56, 128) 0 _________________________________________________________________ conv_pw_3 (Conv2D) (None, 56, 56, 128) 16384 _________________________________________________________________ conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128) 512 _________________________________________________________________ conv_pw_3_relu (ReLU) (None, 56, 56, 128) 0 _________________________________________________________________ conv_pad_4 (ZeroPadding2D) (None, 57, 57, 128) 0 _________________________________________________________________ conv_dw_4 (DepthwiseConv2D) (None, 28, 28, 128) 1152 _________________________________________________________________ conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128) 512 _________________________________________________________________ conv_dw_4_relu (ReLU) (None, 28, 28, 128) 0 _________________________________________________________________ conv_pw_4 (Conv2D) (None, 28, 28, 256) 32768 _________________________________________________________________ conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256) 1024 _________________________________________________________________ conv_pw_4_relu (ReLU) (None, 28, 28, 256) 0 _________________________________________________________________ conv_dw_5 (DepthwiseConv2D) (None, 28, 28, 256) 2304 _________________________________________________________________ conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256) 1024 _________________________________________________________________ conv_dw_5_relu (ReLU) (None, 28, 28, 256) 0 _________________________________________________________________ conv_pw_5 (Conv2D) (None, 28, 28, 256) 65536 _________________________________________________________________ conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256) 1024 _________________________________________________________________ conv_pw_5_relu (ReLU) (None, 28, 28, 256) 0 _________________________________________________________________ conv_pad_6 (ZeroPadding2D) (None, 29, 29, 256) 0 _________________________________________________________________ conv_dw_6 (DepthwiseConv2D) (None, 14, 14, 256) 2304 _________________________________________________________________ conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256) 1024 _________________________________________________________________ conv_dw_6_relu (ReLU) (None, 14, 14, 256) 0 _________________________________________________________________ conv_pw_6 (Conv2D) (None, 14, 14, 512) 131072 _________________________________________________________________ conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_6_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_7 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_7_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_7 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_7_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_8 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_8_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_8 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_8_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_9 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_9_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_9 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_9_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_10_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_10 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_10_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_11_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_11 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_11_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pad_12 (ZeroPadding2D) (None, 15, 15, 512) 0 _________________________________________________________________ conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512) 4608 _________________________________________________________________ conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512) 2048 _________________________________________________________________ conv_dw_12_relu (ReLU) (None, 7, 7, 512) 0 _________________________________________________________________ conv_pw_12 (Conv2D) (None, 7, 7, 1024) 524288 _________________________________________________________________ conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024) 4096 _________________________________________________________________ conv_pw_12_relu (ReLU) (None, 7, 7, 1024) 0 _________________________________________________________________ conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024) 9216 _________________________________________________________________ conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024) 4096 _________________________________________________________________ conv_dw_13_relu (ReLU) (None, 7, 7, 1024) 0 _________________________________________________________________ conv_pw_13 (Conv2D) (None, 7, 7, 1024) 1048576 _________________________________________________________________ conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024) 4096 _________________________________________________________________ conv_pw_13_relu (ReLU) (None, 7, 7, 1024) 0 _________________________________________________________________ global_average_pooling2d (Gl (None, 1024) 0 _________________________________________________________________ reshape_1 (Reshape) (None, 1, 1, 1024) 0 _________________________________________________________________ dropout (Dropout) (None, 1, 1, 1024) 0 _________________________________________________________________ conv_preds (Conv2D) (None, 1, 1, 1000) 1025000 _________________________________________________________________ reshape_2 (Reshape) (None, 1000) 0 _________________________________________________________________ predictions (Activation) (None, 1000) 0 ================================================================= Total params: 4,253,864 Trainable params: 4,231,976 Non-trainable params: 21,888 _________________________________________________________________ . x = mobile.layers[-6].output predictions = Dense(10, activation=&#39;softmax&#39;)(x) . model = Model(inputs=mobile.input, outputs=predictions) . for layer in model.layers[:-23]: layer.trainable = False . model.summary() . Model: &#34;functional_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ conv1_pad (ZeroPadding2D) (None, 225, 225, 3) 0 _________________________________________________________________ conv1 (Conv2D) (None, 112, 112, 32) 864 _________________________________________________________________ conv1_bn (BatchNormalization (None, 112, 112, 32) 128 _________________________________________________________________ conv1_relu (ReLU) (None, 112, 112, 32) 0 _________________________________________________________________ conv_dw_1 (DepthwiseConv2D) (None, 112, 112, 32) 288 _________________________________________________________________ conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32) 128 _________________________________________________________________ conv_dw_1_relu (ReLU) (None, 112, 112, 32) 0 _________________________________________________________________ conv_pw_1 (Conv2D) (None, 112, 112, 64) 2048 _________________________________________________________________ conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64) 256 _________________________________________________________________ conv_pw_1_relu (ReLU) (None, 112, 112, 64) 0 _________________________________________________________________ conv_pad_2 (ZeroPadding2D) (None, 113, 113, 64) 0 _________________________________________________________________ conv_dw_2 (DepthwiseConv2D) (None, 56, 56, 64) 576 _________________________________________________________________ conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64) 256 _________________________________________________________________ conv_dw_2_relu (ReLU) (None, 56, 56, 64) 0 _________________________________________________________________ conv_pw_2 (Conv2D) (None, 56, 56, 128) 8192 _________________________________________________________________ conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128) 512 _________________________________________________________________ conv_pw_2_relu (ReLU) (None, 56, 56, 128) 0 _________________________________________________________________ conv_dw_3 (DepthwiseConv2D) (None, 56, 56, 128) 1152 _________________________________________________________________ conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128) 512 _________________________________________________________________ conv_dw_3_relu (ReLU) (None, 56, 56, 128) 0 _________________________________________________________________ conv_pw_3 (Conv2D) (None, 56, 56, 128) 16384 _________________________________________________________________ conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128) 512 _________________________________________________________________ conv_pw_3_relu (ReLU) (None, 56, 56, 128) 0 _________________________________________________________________ conv_pad_4 (ZeroPadding2D) (None, 57, 57, 128) 0 _________________________________________________________________ conv_dw_4 (DepthwiseConv2D) (None, 28, 28, 128) 1152 _________________________________________________________________ conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128) 512 _________________________________________________________________ conv_dw_4_relu (ReLU) (None, 28, 28, 128) 0 _________________________________________________________________ conv_pw_4 (Conv2D) (None, 28, 28, 256) 32768 _________________________________________________________________ conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256) 1024 _________________________________________________________________ conv_pw_4_relu (ReLU) (None, 28, 28, 256) 0 _________________________________________________________________ conv_dw_5 (DepthwiseConv2D) (None, 28, 28, 256) 2304 _________________________________________________________________ conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256) 1024 _________________________________________________________________ conv_dw_5_relu (ReLU) (None, 28, 28, 256) 0 _________________________________________________________________ conv_pw_5 (Conv2D) (None, 28, 28, 256) 65536 _________________________________________________________________ conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256) 1024 _________________________________________________________________ conv_pw_5_relu (ReLU) (None, 28, 28, 256) 0 _________________________________________________________________ conv_pad_6 (ZeroPadding2D) (None, 29, 29, 256) 0 _________________________________________________________________ conv_dw_6 (DepthwiseConv2D) (None, 14, 14, 256) 2304 _________________________________________________________________ conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256) 1024 _________________________________________________________________ conv_dw_6_relu (ReLU) (None, 14, 14, 256) 0 _________________________________________________________________ conv_pw_6 (Conv2D) (None, 14, 14, 512) 131072 _________________________________________________________________ conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_6_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_7 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_7_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_7 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_7_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_8 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_8_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_8 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_8_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_9 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_9_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_9 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_9_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_10_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_10 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_10_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512) 4608 _________________________________________________________________ conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_dw_11_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pw_11 (Conv2D) (None, 14, 14, 512) 262144 _________________________________________________________________ conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512) 2048 _________________________________________________________________ conv_pw_11_relu (ReLU) (None, 14, 14, 512) 0 _________________________________________________________________ conv_pad_12 (ZeroPadding2D) (None, 15, 15, 512) 0 _________________________________________________________________ conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512) 4608 _________________________________________________________________ conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512) 2048 _________________________________________________________________ conv_dw_12_relu (ReLU) (None, 7, 7, 512) 0 _________________________________________________________________ conv_pw_12 (Conv2D) (None, 7, 7, 1024) 524288 _________________________________________________________________ conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024) 4096 _________________________________________________________________ conv_pw_12_relu (ReLU) (None, 7, 7, 1024) 0 _________________________________________________________________ conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024) 9216 _________________________________________________________________ conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024) 4096 _________________________________________________________________ conv_dw_13_relu (ReLU) (None, 7, 7, 1024) 0 _________________________________________________________________ conv_pw_13 (Conv2D) (None, 7, 7, 1024) 1048576 _________________________________________________________________ conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024) 4096 _________________________________________________________________ conv_pw_13_relu (ReLU) (None, 7, 7, 1024) 0 _________________________________________________________________ global_average_pooling2d (Gl (None, 1024) 0 _________________________________________________________________ dense (Dense) (None, 10) 10250 ================================================================= Total params: 3,239,114 Trainable params: 1,873,930 Non-trainable params: 1,365,184 _________________________________________________________________ . model.compile(optimizer=Adam(lr=0.0001), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . Training . model.fit(x=train_batches, steps_per_epoch=18, validation_data=valid_batches, validation_steps=3, epochs=30, verbose=2) . Epoch 1/30 18/18 - 31s - loss: 0.3288 - accuracy: 0.9302 - val_loss: 0.8580 - val_accuracy: 0.7000 Epoch 2/30 18/18 - 29s - loss: 0.2942 - accuracy: 0.9333 - val_loss: 0.6755 - val_accuracy: 0.7000 Epoch 3/30 18/18 - 21s - loss: 0.2280 - accuracy: 0.9477 - val_loss: 0.4676 - val_accuracy: 0.8000 Epoch 4/30 18/18 - 21s - loss: 0.2050 - accuracy: 0.9722 - val_loss: 0.4056 - val_accuracy: 0.7667 Epoch 5/30 18/18 - 19s - loss: 0.2673 - accuracy: 0.9611 - val_loss: 0.3198 - val_accuracy: 0.9667 Epoch 6/30 18/18 - 17s - loss: 0.1734 - accuracy: 0.9667 - val_loss: 0.0908 - val_accuracy: 1.0000 Epoch 7/30 18/18 - 13s - loss: 0.1649 - accuracy: 0.9500 - val_loss: 0.2169 - val_accuracy: 0.9333 Epoch 8/30 18/18 - 13s - loss: 0.1673 - accuracy: 0.9722 - val_loss: 0.1466 - val_accuracy: 0.9667 Epoch 9/30 18/18 - 9s - loss: 0.1392 - accuracy: 0.9667 - val_loss: 0.1384 - val_accuracy: 0.9333 Epoch 10/30 18/18 - 10s - loss: 0.1067 - accuracy: 0.9833 - val_loss: 0.2402 - val_accuracy: 0.9000 Epoch 11/30 18/18 - 11s - loss: 0.0944 - accuracy: 0.9889 - val_loss: 0.1494 - val_accuracy: 0.9667 Epoch 12/30 18/18 - 8s - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 1.0000 Epoch 13/30 18/18 - 8s - loss: 0.1113 - accuracy: 0.9778 - val_loss: 0.1685 - val_accuracy: 0.9333 Epoch 14/30 18/18 - 7s - loss: 0.0861 - accuracy: 0.9944 - val_loss: 0.0873 - val_accuracy: 1.0000 Epoch 15/30 18/18 - 7s - loss: 0.0628 - accuracy: 0.9942 - val_loss: 0.1392 - val_accuracy: 0.9667 Epoch 16/30 18/18 - 5s - loss: 0.0990 - accuracy: 0.9826 - val_loss: 0.0796 - val_accuracy: 1.0000 Epoch 17/30 18/18 - 4s - loss: 0.0914 - accuracy: 0.9778 - val_loss: 0.0590 - val_accuracy: 1.0000 Epoch 18/30 18/18 - 6s - loss: 0.0534 - accuracy: 0.9944 - val_loss: 0.1309 - val_accuracy: 0.9667 Epoch 19/30 18/18 - 4s - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000 Epoch 20/30 18/18 - 4s - loss: 0.0521 - accuracy: 0.9944 - val_loss: 0.0472 - val_accuracy: 1.0000 Epoch 21/30 18/18 - 3s - loss: 0.0863 - accuracy: 0.9778 - val_loss: 0.0154 - val_accuracy: 1.0000 Epoch 22/30 18/18 - 4s - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9667 Epoch 23/30 18/18 - 4s - loss: 0.0508 - accuracy: 0.9944 - val_loss: 0.0392 - val_accuracy: 1.0000 Epoch 24/30 18/18 - 7s - loss: 0.0607 - accuracy: 0.9944 - val_loss: 0.0256 - val_accuracy: 1.0000 Epoch 25/30 18/18 - 1s - loss: 0.0449 - accuracy: 0.9944 - val_loss: 0.0444 - val_accuracy: 1.0000 Epoch 26/30 18/18 - 2s - loss: 0.0510 - accuracy: 0.9944 - val_loss: 0.0346 - val_accuracy: 1.0000 Epoch 27/30 18/18 - 1s - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 1.0000 Epoch 28/30 18/18 - 2s - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000 Epoch 29/30 18/18 - 2s - loss: 0.0427 - accuracy: 0.9944 - val_loss: 0.0664 - val_accuracy: 0.9667 Epoch 30/30 18/18 - 1s - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0609 - val_accuracy: 1.0000 . &lt;tensorflow.python.keras.callbacks.History at 0x7fd299fcbb00&gt; . Prediction . test_labels = test_batches.classes . predictions = model.predict(x=test_batches, steps=5, verbose=0) . print(predictions) . [[9.99907970e-01 1.81986852e-05 1.14185386e-05 8.31906982e-06 6.76295031e-06 5.26625763e-06 1.50188625e-05 1.29211230e-06 2.20869988e-05 3.54457507e-06] [9.99764740e-01 4.97330475e-05 1.24770275e-04 1.25518391e-05 2.54034444e-06 1.78213008e-06 2.27403270e-05 5.41358190e-07 1.18345215e-05 8.72721012e-06] [9.99943495e-01 1.02443319e-05 4.07669768e-06 2.59417743e-06 2.22588847e-06 1.97174950e-06 4.29789634e-06 2.17091639e-07 3.33743219e-06 2.75765979e-05] [9.97976959e-01 1.30962511e-03 4.67506397e-05 4.97402689e-05 1.01239813e-04 3.53960495e-05 3.83031263e-04 4.16261264e-06 1.39273197e-05 7.91871498e-05] [9.99794781e-01 6.55109470e-05 5.60145490e-05 9.19256217e-06 6.74312616e-07 2.55389045e-06 1.12290845e-05 3.24747077e-07 6.57366490e-06 5.31644619e-05] [5.96624841e-06 9.98959184e-01 9.82636935e-04 4.61979107e-06 1.41901154e-07 3.83149796e-08 2.53536768e-06 1.55028567e-06 3.95551178e-05 3.84179430e-06] [5.77348619e-05 9.97837484e-01 1.93230971e-03 5.29086901e-05 5.60771980e-07 1.32886186e-07 1.02190063e-06 7.83198743e-07 9.52457194e-05 2.19350532e-05] [3.19325909e-06 9.96406376e-01 3.52200959e-03 2.73286496e-05 1.38922232e-07 8.74872441e-08 1.08019390e-06 7.19391437e-07 3.41323175e-05 4.93078778e-06] [8.91397633e-07 9.99843597e-01 6.41833540e-05 9.20376624e-06 5.28325529e-07 2.35107791e-07 4.17812544e-06 7.52144786e-07 6.69909277e-05 9.46024920e-06] [2.11145539e-06 9.99277651e-01 6.38888392e-04 1.61246521e-06 8.37773797e-08 1.33889078e-08 1.29828607e-07 2.36027589e-07 6.06195317e-05 1.86159541e-05] [1.40882321e-06 3.04088953e-05 9.99190271e-01 7.30123953e-04 3.74597857e-06 7.04620806e-09 1.89693892e-05 5.52513166e-06 3.02043100e-06 1.65276870e-05] [4.09996355e-05 1.62411539e-03 9.77828205e-01 8.13836232e-04 3.26748923e-05 2.13361272e-06 1.89798046e-02 5.88272000e-04 2.49287514e-05 6.49938665e-05] [8.36342861e-06 3.36779805e-04 9.93821859e-01 6.53320982e-04 2.57492866e-05 3.32229831e-07 5.12548909e-03 1.22495271e-06 9.05711147e-07 2.60038087e-05] [7.99389818e-06 2.90314310e-05 9.94603097e-01 1.44842779e-03 7.65812001e-05 4.65628233e-07 3.05270427e-03 5.67832671e-04 7.49530227e-05 1.38985488e-04] [3.46522438e-06 4.86433157e-04 9.99253333e-01 1.86141515e-05 8.96118217e-06 4.99642780e-08 2.13554318e-04 4.10330898e-07 1.25275583e-05 2.64599180e-06] [5.68577816e-05 2.32892362e-05 1.61046904e-04 9.97760653e-01 1.23008670e-06 1.79811893e-03 1.02986924e-05 1.30777389e-05 1.74625966e-04 8.17895113e-07] [3.42618478e-05 9.89843393e-05 1.23947440e-02 9.87302780e-01 4.45760946e-07 1.29584689e-04 1.45311124e-05 1.34755164e-05 3.32886202e-06 7.85242901e-06] [2.61722424e-04 2.25970231e-04 5.14747517e-04 9.92549181e-01 9.46297405e-06 5.28588658e-03 5.42270891e-06 1.30143080e-05 1.05099462e-03 8.35767059e-05] [1.31443335e-06 5.72117733e-06 5.44280512e-04 9.99418259e-01 9.58634132e-08 2.24492705e-05 6.64082506e-07 4.97399469e-07 5.72592035e-06 1.02367096e-06] [7.33219276e-05 3.56858254e-05 2.46638752e-04 7.77239680e-01 2.80055178e-06 2.20447138e-01 1.29028433e-06 1.87667436e-04 5.07000514e-05 1.71502493e-03] [8.84616838e-06 1.72287150e-06 5.27130305e-06 6.36111281e-07 9.97579277e-01 1.48030219e-03 8.55612045e-04 6.84547058e-06 2.29201742e-05 3.85024141e-05] [2.87977709e-05 5.12899396e-06 2.86878530e-05 8.23870141e-06 9.94071543e-01 2.25713290e-03 3.38535896e-03 8.99201550e-05 4.08533342e-05 8.42990703e-05] [6.81434831e-05 1.86656780e-06 5.61141642e-04 5.19602145e-05 9.81923401e-01 3.11230286e-03 1.35045694e-02 2.41562775e-05 2.61299167e-04 4.91183600e-04] [6.54945097e-06 3.00924683e-07 2.14229894e-05 2.13994663e-05 9.87928689e-01 9.72454902e-04 1.10056009e-02 2.72963043e-05 6.87222837e-06 9.38202265e-06] [5.10706229e-07 1.95524965e-08 1.11063594e-06 1.30337980e-07 9.99499679e-01 2.78539665e-04 2.16483604e-04 3.37662101e-07 1.16396768e-06 1.94427730e-06] [1.10659057e-05 1.02498973e-07 2.80285690e-07 2.31634567e-06 1.32311597e-01 8.67163122e-01 4.84139746e-05 4.81770039e-05 8.51955556e-05 3.29826580e-04] [3.36566040e-06 2.71774951e-07 1.56815133e-07 1.89455866e-04 1.66468462e-03 9.98027265e-01 4.27184023e-06 1.02549639e-05 1.09216107e-05 8.93709075e-05] [7.34167543e-06 1.22745541e-06 1.68236136e-06 6.73042436e-04 7.15185364e-04 9.98450279e-01 2.56239873e-05 1.11202444e-05 7.43143682e-05 4.00367535e-05] [3.26074020e-04 1.73902168e-04 3.35095356e-05 2.35643201e-02 4.65085953e-02 9.28241014e-01 1.87676211e-04 1.17491370e-04 6.12921838e-04 2.34515188e-04] [1.53196670e-05 2.26287568e-07 6.14817793e-07 3.94643546e-04 7.02123623e-04 9.98710632e-01 1.38914984e-05 2.12150462e-06 1.51932225e-04 8.39336371e-06] [1.36404287e-03 1.38173311e-03 1.00178637e-01 7.36371102e-03 3.09866350e-02 3.71698756e-03 7.34847426e-01 7.62851462e-02 3.38587798e-02 1.00168865e-02] [1.55438215e-06 7.56569671e-06 1.93705852e-03 1.57460931e-06 1.83093990e-03 1.72299588e-06 9.96193767e-01 2.21732480e-05 1.18845605e-06 2.45226647e-06] [1.73982175e-03 2.90282245e-04 2.19090790e-01 3.85022792e-03 9.29833874e-02 3.65315960e-03 6.71241343e-01 5.50376344e-03 9.16578050e-04 7.30645843e-04] [1.66806840e-05 4.53790699e-06 5.32073714e-03 9.55705673e-06 1.46365270e-03 6.23514643e-05 9.92924690e-01 1.48642837e-04 4.59050279e-05 3.24408575e-06] [4.00441844e-04 8.47149786e-05 8.38202331e-03 3.48406640e-04 3.25773731e-02 9.81657766e-04 9.54741716e-01 1.99468597e-03 7.89099722e-05 4.10044391e-04] [1.42006917e-04 6.20908104e-04 7.04250718e-03 5.62605681e-03 1.17695192e-03 2.23223586e-04 4.37225390e-04 5.33069670e-01 4.49340671e-01 2.32081162e-03] [1.46449963e-06 2.21280516e-06 1.03685644e-03 6.59345533e-05 9.58273071e-04 1.10456494e-05 2.16003522e-04 9.95881200e-01 1.73346093e-03 9.36524666e-05] [1.57869945e-04 4.90538543e-04 7.90488794e-02 1.33991949e-02 8.82933266e-04 4.67371312e-04 2.89049931e-03 9.00509000e-01 1.39442889e-03 7.59232207e-04] [2.76261676e-06 3.58992293e-06 4.14676004e-04 4.83516169e-06 6.32872479e-06 1.87327737e-06 6.94003984e-06 9.98579502e-01 9.59948462e-04 1.96528272e-05] [1.15150295e-03 4.81685682e-04 2.58751353e-03 8.08459241e-04 5.00243269e-02 8.77166225e-04 2.17872718e-03 3.35247427e-01 6.00740671e-01 5.90256089e-03] [3.27213456e-06 1.54394394e-04 7.79893016e-05 4.01407488e-05 2.11231927e-05 5.19559671e-06 4.20569876e-07 1.38790475e-03 9.97655511e-01 6.54020463e-04] [2.57532171e-04 8.40583307e-05 1.26030098e-03 5.38015622e-04 1.50475139e-03 1.51385684e-04 1.82284115e-04 8.22080001e-02 9.12585676e-01 1.22801040e-03] [8.28076227e-05 1.24441576e-04 2.39340728e-03 1.02791074e-03 1.78400415e-03 1.56045702e-04 6.67873828e-05 6.77036028e-03 9.84457195e-01 3.13709350e-03] [3.07454411e-06 6.68912617e-06 1.13914311e-05 9.11148436e-06 1.44990317e-05 7.82387178e-06 3.69191838e-07 3.12103220e-04 9.99610364e-01 2.45070169e-05] [1.30470420e-04 1.23008965e-02 2.51298794e-03 1.70998927e-03 2.84471782e-04 4.16153198e-05 7.09045635e-05 1.25928685e-01 8.49215686e-01 7.80425733e-03] [4.43604549e-05 3.70138223e-05 4.58232216e-06 4.78399634e-05 7.25927763e-04 4.10610024e-04 4.00512499e-06 4.73862128e-05 8.23822047e-04 9.97854412e-01] [8.90586991e-04 2.65139650e-04 4.86360630e-03 4.50516725e-03 2.76033883e-03 4.42838715e-03 2.65056326e-04 1.55687588e-03 3.97594035e-01 5.82870781e-01] [3.13964701e-04 2.22514194e-04 2.69785814e-04 3.42328494e-05 9.21714026e-03 8.76264356e-04 5.47206611e-04 1.68818122e-04 4.44504339e-03 9.83905017e-01] [7.43059732e-04 1.87608050e-04 3.47952708e-04 8.38607200e-04 1.96068548e-03 2.17008434e-04 1.08727392e-04 8.90582232e-05 1.37780723e-03 9.94129539e-01] [1.50213436e-05 2.07497487e-05 2.58430646e-05 5.99290024e-06 4.50582767e-04 1.80798364e-04 1.50254700e-05 2.42412762e-06 1.96153924e-04 9.99087453e-01]] . cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1)) . def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . cm_plot_labels = [&#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;] plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=&#39;Confusion Matrix&#39;) . Confusion matrix, without normalization [[5 0 0 0 0 0 0 0 0 0] [0 5 0 0 0 0 0 0 0 0] [0 0 5 0 0 0 0 0 0 0] [0 0 0 5 0 0 0 0 0 0] [0 0 0 0 5 0 0 0 0 0] [0 0 0 0 0 5 0 0 0 0] [0 0 0 0 0 0 5 0 0 0] [0 0 0 0 0 0 0 4 1 0] [0 0 0 0 0 0 0 0 5 0] [0 0 0 0 0 0 0 0 0 5]] .",
            "url": "https://saptarshidatta.in/2020/09/08/Sign-Language-Prediction.html",
            "relUrl": "/2020/09/08/Sign-Language-Prediction.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Sign Language Prediction with MobileNet - Literature",
            "content": "In the last post, we applied Transfer Learning in the VGG-16 Model with the Cat vs Dogs data set. However the application was minimal as we only changed the last output layer from a ‘softmax’ actiavted outpur to a ‘sigmoid’ activated output. Additionally, the VGG-16 model was already trained on the ImageNet Data, which originally had imaged of cats and dogs. We jist trained the last dense layer which predicted whether the image is og a cat or a dog. . However, in this exercise we shall again apply transfer learning to predict the Numeric Sign Language. We will be applying MobileNet Model and shall modify the model and then fine tune it to suit our requirements. But before that, let’s discuss a bit about the MobileNet Model. . MobileNet Model . MobileNets are a class of small, low-latency and low-power model that can be used for classification, detection, and other common tasks convolutional neural networks are good for. Because of their small size, these are considered great deep learning models to be used on mobile devices. . The MobileNet is about 17 MB in size and has just 4.2 million parameters as compared to the VGG-16 model which has a size of 534 MB and around 138 Million parameters. . However due to their smaller size and faster performance than other networks, MobileNets aren’t as accurate as the other large, resource-heavy models. However they still actually perform very well, with really only a relatively small reduction in accuracy. . Please go through the MobileNet Paper which elaborates further regarding the tradeoff between accuracy and size. . Having discussed the MobileNet model to some extent, let move ahead to other sections. Alright, let’s jump into the code! . Data Preparation . We have used the Sign Language Digits dataset from GitHub. The data is located in corresponding folders ranging from 0-9, however we will use a script to divide the data into train, test and valid datasets. . os.chdir(&#39;/content/gdrive/My Drive/Sign-Language-Digits-Dataset/Dataset&#39;) if os.path.isdir(&#39;train/0/&#39;) is False: os.mkdir(&#39;train&#39;) os.mkdir(&#39;valid&#39;) os.mkdir(&#39;test&#39;) for i in range(0, 10): shutil.move(f&#39;{i}&#39;, &#39;train&#39;) os.mkdir(f&#39;valid/{i}&#39;) os.mkdir(f&#39;test/{i}&#39;) valid_samples = random.sample(os.listdir(f&#39;train/{i}&#39;), 30) for j in valid_samples: shutil.move(f&#39;train/{i}/{j}&#39;, f&#39;valid/{i}&#39;) test_samples = random.sample(os.listdir(f&#39;train/{i}&#39;), 5) for k in test_samples: shutil.move(f&#39;train/{i}/{k}&#39;, f&#39;test/{i}&#39;) os.chdir(&#39;../..&#39;) . So what we are basically doing in the above script is at first checking whether a train folder already exists, if not, we are creating a train/test/valid folders. Then, we will move all the images corresponding to a particular class-folder from the main folder to the corresponding class-folder inside the train folder, and at the same time creating new class-folder inside the valid and test folders. Then we randomly move 30 and 5 images from each class-folder inside train folder to the corresponding class-folders inside valid and test folders that were created before. We run the entire process in a loop iterating over class-folders ranging from 0-9. . Next, we preprocess the train, valid and test data in a fashion the MobileNet model expects(MobileNet expects images to be scales between [-1,1] rather than [0, 225]. We set the batch size to 10 and the target image size to (224, 224) since that is the image size that MobileNet expects. We set shuffle to False for the test batch, so that later we can plot our results on to a confusion Matrix. . train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory( directory=train_path, target_size=(224,224), batch_size=10) valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory( directory=valid_path, target_size=(224,224), batch_size=10) test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory( directory=test_path, target_size=(224,224), batch_size=10, shuffle=False) . Fine Tuning the MobileNet Model . We will import the MobileNet Model just as we imported the VGG-16 Model. Then we will drop the last 5 layers forom the model and add a dense layer with softmax activation predicting 10 classes ranging from 0-9. Later we freeze all the layers except the last 23 layers (A MobileNet Model has 88 Layers). The choice of the number 23 is based upon personal choice and some experimentation. It was found out that if we train the last 23 layers, we get some really good results. Please note that it is a significant deviation from the last VGG-16 Model training, where we only trained the last output layer. . mobile = tf.keras.applications.mobilenet.MobileNet() x = mobile.layers[-6].output predictions = Dense(10, activation=&#39;softmax&#39;)(x) model = Model(inputs=mobile.input, outputs=predictions) for layer in model.layers[:-23]: layer.trainable = False . Now, we will compile and train the model for 30 epochs and our model scores 100% valodation accuracy and 98.89% train accuracy. This shows taht our model is generalising well. . model.compile(optimizer=Adam(lr=0.0001), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(x=train_batches, steps_per_epoch=18, validation_data=valid_batches, validation_steps=3, epochs=30, verbose=2) . Here are the accuracies for the last 5 epochs: . Epoch 25/30 18/18 - 1s - loss: 0.0449 - accuracy: 0.9944 - val_loss: 0.0444 - val_accuracy: 1.0000 Epoch 26/30 18/18 - 2s - loss: 0.0510 - accuracy: 0.9944 - val_loss: 0.0346 - val_accuracy: 1.0000 Epoch 27/30 18/18 - 1s - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 1.0000 Epoch 28/30 18/18 - 2s - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 1.0000 Epoch 29/30 18/18 - 2s - loss: 0.0427 - accuracy: 0.9944 - val_loss: 0.0664 - val_accuracy: 0.9667 Epoch 30/30 18/18 - 1s - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0609 - val_accuracy: 1.0000 &lt;tensorflow.python.keras.callbacks.History at 0x7fd299fcbb00&gt; . Prediction on the Test Batch . Now, let’s test the model on the test batch and plot the confusion matrix. . predictions = model.predict(x=test_batches, steps=5, verbose=0) cm = confusion_matrix(y_true=test_labels, y_pred=predictions.argmax(axis=1)) cm_plot_labels = [&#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;] plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=&#39;Confusion Matrix&#39;) Confusion matrix, without normalization [[5 0 0 0 0 0 0 0 0 0] [0 5 0 0 0 0 0 0 0 0] [0 0 5 0 0 0 0 0 0 0] [0 0 0 5 0 0 0 0 0 0] [0 0 0 0 5 0 0 0 0 0] [0 0 0 0 0 5 0 0 0 0] [0 0 0 0 0 0 5 0 0 0] [0 0 0 0 0 0 0 4 1 0] [0 0 0 0 0 0 0 0 5 0] [0 0 0 0 0 0 0 0 0 5]] . Our model has performed excellent on the Test batch with only one error. Please view the published code associated with this post. .",
            "url": "https://saptarshidatta.in/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html",
            "relUrl": "/2020/09/08/Sign-Language-Digit-Prediction-with-Mobile-Net.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Transfer Learning with VGG-16",
            "content": "Training large models with huge number of trainable parameters is often time consuming. Sometimes, model training may go on for a couple of days or even more. In that case, if after such long training period, if the moel does not perform suitably, we again need to modify and re-train the model for such longer duration. This phenomenon results in wastege of time and efforts. . We do have a technique called Transfer Learning for quicker model training and hence faster experimentation. Various pretrained models like VGG-16, VGG-19, ResNet etc. are available with the weights of their corrspondinhg layers. We need to use the weights of the earlier layers and onlu need to train the outer layers with the output layers. This is because, most of the models determine simple features at their initial/earlier layers which are same for most of the tasks. Hence, we do not need to train them again. . We have prepared a notebook to answer a problem where we try to distinguish between a cat and a dog. We first implement our own Sequential model, train it and note down the accuracy. Then, we implement a VGG-16 Model with all pretrained parameters. We remove the last/output layer which initially has a softmax activation of 1000 outputs and replace it by a sigmoid activation for predicting a cat or a dog. We will train only the last layer and keep the parameters of the previous layers as it is. . Logically, I don’t see any real life benefit by solving the above problem except understanding the benefits of Transfer Learning. That is what, I specifically want to show with this exercise. . Our own sequential model . The first is the convolution (Conv2D) layer. It is like a set of learnable filters. We chose to set 64 filters for the two firsts Conv2D layers and 64 filters for the lat one. Each filter transforms a part of the image as defined by the kernel size(3x3) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters are used to transform the image. . The CNN can isolate features that are useful everywhere from these transformed images (feature maps). . The second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a down sampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduces over fitting. We chose the pooling size as (2x2). . Combining convolution and pooling layers, CNN are able to combine local features and learn more global features of the image. . Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the over fitting. . ‘relu’ is the rectified linear activation function. Mathematically it can be defined as . f(x) = max(0,x) . The activation function is used to add non linearity to the network. . The Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolution/maxpool layers. It combines all the found local features of the previous convolution layers. . In the end, two fully-connected (Dense) layers are used which are just artificial an neural networks (ANN). In the last dense layer, ‘sigmoid’ activation is used to predict whether the input image belongs to a cat or a dog. . model= tf.keras.models.Sequential( [tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;, input_shape = (224,224,3)), tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(.25), tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)] . The above model 23,963,777 trainable parameters and takes around 7 minutes to train per epoch. . model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 222, 222, 64) 1792 _________________________________________________________________ conv2d_1 (Conv2D) (None, 220, 220, 64) 36928 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 110, 110, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 110, 110, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 108, 108, 64) 36928 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 186624) 0 _________________________________________________________________ dense (Dense) (None, 128) 23888000 _________________________________________________________________ dense_1 (Dense) (None, 1) 129 ================================================================= Total params: 23,963,777 Trainable params: 23,963,777 Non-trainable params: 0 _________________________________________________________________ . Since the model takes around 7 minutes to train per epoch, we trained the model for only two epochs. . model.fit(x=train_batches, steps_per_epoch=len(train_batches), validation_data=valid_batches, validation_steps=len(valid_batches), epochs=2, verbose=2 ) Epoch 1/2 182/182 - 344s - loss: 0.6734 - accuracy: 0.5904 - val_loss: 0.6204 - val_accuracy: 0.6984 Epoch 2/2 182/182 - 332s - loss: 0.5923 - accuracy: 0.6782 - val_loss: 0.5587 - val_accuracy: 0.7304 &lt;tensorflow.python.keras.callbacks.History at 0x7fa93117e550&gt; . We find that both the train &amp; validation accuracy is not that great. It’s only around 69% to start with. You may train the model with longer epochs if time is not a constraint. Now, let’s implment a VGG-16 pre-trained Model. . VGG-16 pre-trained Model . At first, we import the model. We need to have internet connection to import the model. . vgg16_model = tf.keras.applications.vgg16.VGG16() Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5 553467904/553467096 [==============================] - 5s 0us/step . Now, let’s summrise the imported version of the VGG-16 Model. . vgg16_model.summary() Model: &quot;vgg16&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 224, 224, 3)] 0 _________________________________________________________________ block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ predictions (Dense) (None, 1000) 4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________ . We can see that teh VGG-16 modeel has 138,357,544 trainable parameters, but as said above, we are not going to train all of them. Instead, we are going to remove the last dense layer used for predictions, set rest of the layers as non-trainable and then add a sigmoid activation dense prediction layer with output shape (None, 1). . model = Sequential() for layer in vgg16_model.layers[:-1]: model.add(layer) for layer in model.layers: layer.trainable = False model.add(Dense(units=1, activation=&#39;sigmoid&#39;)) . Next, we print the summary of the model. . model.summary() Model: &quot;sequential_1&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 _________________________________________________________________ block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 _________________________________________________________________ block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 _________________________________________________________________ block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 _________________________________________________________________ block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 _________________________________________________________________ block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 _________________________________________________________________ block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 _________________________________________________________________ block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 _________________________________________________________________ block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 _________________________________________________________________ block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 _________________________________________________________________ block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 _________________________________________________________________ block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 _________________________________________________________________ block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 _________________________________________________________________ block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 _________________________________________________________________ flatten (Flatten) (None, 25088) 0 _________________________________________________________________ fc1 (Dense) (None, 4096) 102764544 _________________________________________________________________ fc2 (Dense) (None, 4096) 16781312 _________________________________________________________________ dense_2 (Dense) (None, 1) 4097 ================================================================= Total params: 134,264,641 Trainable params: 4,097 Non-trainable params: 134,260,544 . We observe that, only 4097 parameters are trainable. This will considerably improve the training speed along with the accuracy of the model. We will see this next. . model.fit(x=train_batches1, steps_per_epoch=len(train_batches1), validation_data=valid_batches1, validation_steps=len(valid_batches1), epochs=3, verbose=2) Epoch 1/3 182/182 - 114s - loss: 0.2074 - accuracy: 0.9144 - val_loss: 0.0748 - val_accuracy: 0.9764 Epoch 2/3 182/182 - 112s - loss: 0.0647 - accuracy: 0.9785 - val_loss: 0.0541 - val_accuracy: 0.9800 Epoch 3/3 182/182 - 111s - loss: 0.0519 - accuracy: 0.9826 - val_loss: 0.0470 - val_accuracy: 0.9832 &lt;tensorflow.python.keras.callbacks.History at 0x7fa930e68d50&gt; . We see that the training time per epoch is 114 seconds instead of 344 seconds previously. Also the train and validation accuracy is around 98% to start with. Please train the model for 30 epochs to get acuracy around 99.6% Hence, we can say that using pre-trained models signifcantly improves the accuracy as well as reduces the training time. . Some Additional Information . The pre-trained model we fine-tuned to classify images of cats and dogs is called VGG16, which is the model that won the 2014 ImageNet competition. . In the ImageNet competition, multiple teams compete to build a model that best classifies images from the ImageNet library. The ImageNet library houses thousands of images belonging to 1000 different categories. . Note that dogs and cats were included in the ImageNet library from which VGG16 was originally trained. Therefore, the model has already learned the features of cats and dogs. Given this, the fine-tuning we’ve done on this model is very minimal. In future blogs, we’ll do more involved fine-tuning and utilize transfer learning to classify completely new data than what was included in the original training set. . In this post, we have discussed only about Transfer Learning. However, we have not discussed about preprocessing the data that is required for transfer learning. For VGG-16 model, we have preprocessed the image in a certain way as it should be for VGG-16 model training. Further details about this is explained in the VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION paper. . Please access the GitHub repository to view the code. . © 2020 Saptarshi Datta. All Rights Reserved. .",
            "url": "https://saptarshidatta.in/2020/09/06/Transfer-Learning-with-VGG-16.html",
            "relUrl": "/2020/09/06/Transfer-Learning-with-VGG-16.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "MNIST Digit Recognizer Code",
            "content": "from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/gdrive . import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg import seaborn as sns %matplotlib inline np.random.seed(2) from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix import itertools import os import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D from tensorflow.keras.optimizers import Adam from tensorflow.keras.metrics import categorical_crossentropy from tensorflow.keras.preprocessing.image import ImageDataGenerator sns.set(style=&#39;white&#39;, context=&#39;notebook&#39;, palette=&#39;deep&#39;) . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . Load the data . train = pd.read_csv(&#39;/content/gdrive/My Drive/digit-recognizer/train.csv&#39;) test = pd.read_csv(&#39;/content/gdrive/My Drive/digit-recognizer/test.csv&#39;) print(train.shape) print(test.shape) . (42000, 785) (28000, 784) . Y_train = train[&#39;label&#39;] X_train =train.drop(labels = [&#39;label&#39;], axis = 1) . Checking the distribution of the train data . sns.countplot(Y_train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdff922d828&gt; . Testing for Null and Missing values . X_train.isnull().any() . pixel0 False pixel1 False pixel2 False pixel3 False pixel4 False ... pixel779 False pixel780 False pixel781 False pixel782 False pixel783 False Length: 784, dtype: bool . test.isnull().any() . pixel0 False pixel1 False pixel2 False pixel3 False pixel4 False ... pixel779 False pixel780 False pixel781 False pixel782 False pixel783 False Length: 784, dtype: bool . Normalize the Data . x_train = X_train / 255 test = test / 255 . Reshape . X_train = X_train.values.reshape(-1, 28, 28, 1) test = test.values.reshape(-1, 28, 28, 1) . Label Encoding . Y_train = tf.keras.utils.to_categorical(Y_train, num_classes = 10) . Split the Training into Training/Test Set . X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2) . g = plt.imshow(X_train[1][:,:,0]) . CNN Model . model= tf.keras.models.Sequential( [tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;, input_shape = (28,28,1)), tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(.25), tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)] ) . model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ conv2d_1 (Conv2D) (None, 24, 24, 64) 36928 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 12, 12, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 10, 10, 64) 36928 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 1600) 0 _________________________________________________________________ dense (Dense) (None, 128) 204928 _________________________________________________________________ dense_1 (Dense) (None, 10) 1290 ================================================================= Total params: 280,714 Trainable params: 280,714 Non-trainable params: 0 _________________________________________________________________ . Without Data augmentation, Validation accuracy = 98.98% . model.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=100, epochs=24, verbose=2) . Epoch 1/24 WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0072s). Check your callbacks. 378/378 - 3s - loss: 0.6462 - accuracy: 0.9002 - val_loss: 0.0789 - val_accuracy: 0.9743 Epoch 2/24 378/378 - 3s - loss: 0.0802 - accuracy: 0.9747 - val_loss: 0.0541 - val_accuracy: 0.9826 Epoch 3/24 378/378 - 3s - loss: 0.0576 - accuracy: 0.9822 - val_loss: 0.0570 - val_accuracy: 0.9807 Epoch 4/24 378/378 - 3s - loss: 0.0437 - accuracy: 0.9862 - val_loss: 0.0457 - val_accuracy: 0.9850 Epoch 5/24 378/378 - 3s - loss: 0.0382 - accuracy: 0.9885 - val_loss: 0.0463 - val_accuracy: 0.9857 Epoch 6/24 378/378 - 3s - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.0548 - val_accuracy: 0.9852 Epoch 7/24 378/378 - 3s - loss: 0.0298 - accuracy: 0.9899 - val_loss: 0.0396 - val_accuracy: 0.9888 Epoch 8/24 378/378 - 3s - loss: 0.0269 - accuracy: 0.9911 - val_loss: 0.0469 - val_accuracy: 0.9864 Epoch 9/24 378/378 - 3s - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0492 - val_accuracy: 0.9860 Epoch 10/24 378/378 - 3s - loss: 0.0219 - accuracy: 0.9932 - val_loss: 0.0482 - val_accuracy: 0.9871 Epoch 11/24 378/378 - 3s - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.0354 - val_accuracy: 0.9912 Epoch 12/24 378/378 - 3s - loss: 0.0188 - accuracy: 0.9940 - val_loss: 0.0487 - val_accuracy: 0.9883 Epoch 13/24 378/378 - 3s - loss: 0.0239 - accuracy: 0.9925 - val_loss: 0.0493 - val_accuracy: 0.9881 Epoch 14/24 378/378 - 3s - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0456 - val_accuracy: 0.9900 Epoch 15/24 378/378 - 3s - loss: 0.0167 - accuracy: 0.9947 - val_loss: 0.0480 - val_accuracy: 0.9890 Epoch 16/24 378/378 - 3s - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.0424 - val_accuracy: 0.9900 Epoch 17/24 378/378 - 3s - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.0487 - val_accuracy: 0.9881 Epoch 18/24 378/378 - 3s - loss: 0.0174 - accuracy: 0.9947 - val_loss: 0.0624 - val_accuracy: 0.9869 Epoch 19/24 378/378 - 3s - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0492 - val_accuracy: 0.9905 Epoch 20/24 378/378 - 3s - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0533 - val_accuracy: 0.9871 Epoch 21/24 378/378 - 3s - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0422 - val_accuracy: 0.9910 Epoch 22/24 378/378 - 3s - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.0484 - val_accuracy: 0.9900 Epoch 23/24 378/378 - 3s - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.0489 - val_accuracy: 0.9888 Epoch 24/24 378/378 - 3s - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0484 - val_accuracy: 0.9907 . &lt;tensorflow.python.keras.callbacks.History at 0x7fdff8f17a90&gt; . model.save(&#39;/content/gdrive/My Drive/digit-recognizer/digit_recognizer_trial_model.h5&#39;) . Confusion Matrix . def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) # Predict the values from the validation dataset Y_pred = model.predict(X_test) # Convert predictions classes to one hot vectors Y_pred_classes = np.argmax(Y_pred,axis = 1) # Convert validation observations to one hot vectors Y_true = np.argmax(Y_test,axis = 1) # compute the confusion matrix confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) # plot the confusion matrix plot_confusion_matrix(confusion_mtx, classes = range(10)) . results = model.predict(test) # select the indix with the maximum probability results = np.argmax(results,axis = 1) results = pd.Series(results,name=&quot;Label&quot;) . submission = pd.concat([pd.Series(range(1,28001),name = &quot;ImageId&quot;),results],axis = 1) submission.to_csv(&quot;/content/gdrive/My Drive/digit-recognizer/cnn_mnist_datagen.csv&quot;,index=False) .",
            "url": "https://saptarshidatta.in/2020/08/31/Digit-Recognizer.html",
            "relUrl": "/2020/08/31/Digit-Recognizer.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Digit Recognizer with CNN - Literature",
            "content": "MNIST (“Modified National Institute of Standards and Technology”) is the de-facto “hello world” data set of computer vision. Since its release in 1999, this classic data set of handwritten images has served as the basis for bench marking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. . In this exercise, the goal is to correctly identify digits from a data set of tens of thousands of handwritten images. . This Notebook contains three main parts: . -The data preparation . -The CNN modeling and evaluation . -The results prediction and analysis . Data Preparation . The Google Drive is mounted on the Google Colab Notebook. The train &amp; test data is then loaded using the well known pandas read_csv(). The train set 42000 rows and 785 columns( including labels) and the test set has 28000 rows and 784 columns. As known, the test set is not supposed to have the labels. . After importing the data sets, the label column from the train set is saved as another data frame, where as it is dropped from the original train set. This is merely done due to input requirements of the fit() function which takes the data &amp; the labels separately during the model training phase. . Distribution of Training data among Classes . The training data seems to be distributed equally among the classes ranging from 0 - 9. . Checking for Null Data . We checked for null data, but did not find any. No further actions taken in this regard. . Normalization . We perform a gray scale normalization to reduce the effect of illumination’s differences. Moreover the CNN converge faster when pixel values ranges in [0, 1] rather than on [0, 255]. . Reshape . Train and test images has been stock into pandas dataframe as 1D vectors of 784 values. We reshape all data to (28x28x1) 3D matrices. Keras(with TensorFlow back end) requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so it use only one channel. Had it been colored or RGB images containing 3 channels each corresponding to Red, Green &amp; Blue, we would have reshaped 784 vectors to (28x28x3) 3D matrices. . Label Encoding . Labels are 10 digits numbers from 0 to 9. We need to encode these labels to one hot vectors. . Splitting the Train Set . Next we split the train set into train and test in the ratio of 9:1. This 10% set will act as the validation set, to prevent our model from over fitting on the train set. . CNN Modelling &amp; Evaluation . The first is the convolution (Conv2D) layer. It is like a set of learnable filters. We chose to set 64 filters for the two firsts Conv2D layers and 64 filters for the lat one. Each filter transforms a part of the image as defined by the kernel size(3x3) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters are used to transform the image. . The CNN can isolate features that are useful everywhere from these transformed images (feature maps). . The second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a down sampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduces over fitting. We chose the pooling size as (2x2). . Combining convolution and pooling layers, CNN are able to combine local features and learn more global features of the image. . Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the over fitting. . ‘relu’ is the rectified linear activation function Mathematically it can be defined as f(x) = max(0,x). The activation function is used to add non linearity to the network. . The Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolution/maxpool layers. It combines all the found local features of the previous convolution layers. . In the end, two fully-connected (Dense) layers are used which are just artificial an neural networks (ANN). In the last dense layer, ‘softmax’ activation is used which is used to output distribution of probability of each class. . Here is the Sequential model created: . model= tf.keras.models.Sequential( [tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;, input_shape = (28,28,1)), tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;), tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Dropout(.25), tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = &#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)] ) . Loss Function &amp; Optimizer . We choose the loss function as ‘categorical cross entropy’ as the number of categorical classifications is more than 2. The loss function is the error rate between the observed labels and the predicted ones. . We used the Adam Optimizer for this particular problem. . Model Summary . model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 26, 26, 64) 640 _________________________________________________________________ conv2d_1 (Conv2D) (None, 24, 24, 64) 36928 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 12, 12, 64) 0 _________________________________________________________________ dropout (Dropout) (None, 12, 12, 64) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 10, 10, 64) 36928 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 1600) 0 _________________________________________________________________ dense (Dense) (None, 128) 204928 _________________________________________________________________ dense_1 (Dense) (None, 10) 1290 ================================================================= Total params: 280,714 Trainable params: 280,714 Non-trainable params: 0 _________________________________________________________________ . Train the model . The fit function is responsible for training the model. It takes as input the train data as x and the labels as y. In addition to this, we use a validation set as discussed earlier. In addition to this choose batch size as 100, epochs = 24 and verbose = 2. We trained the model using the GPU available on Google Colab. . model.fit(X_train, Y_train,validation_data=(X_test, Y_test), batch_size=100, epochs=24, verbose=2) . Result Prediction &amp; Analysis . After training the model, at the 24th epoch, the training accuracy is 99.8% and the validation accuracy is 98.9%. This shows that our model did not over fit much. Accuracy can be further improved if Data Augmentation is performed. . Confusion Matrix . We plot the confusion matrix and found out some errors made during classification task performed on the validation set. It seems that our CNN has some little troubles with the 4 digits, hey are mis-classified as 9. Sometime it is very difficult to catch the difference between 4 and 9 when curves are smooth. . def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): &quot;&quot;&quot; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. &quot;&quot;&quot; plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) # Predict the values from the validation dataset Y_pred = model.predict(X_test) # Convert predictions classes to one hot vectors Y_pred_classes = np.argmax(Y_pred,axis = 1) # Convert validation observations to one hot vectors Y_true = np.argmax(Y_test,axis = 1) # compute the confusion matrix confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) # plot the confusion matrix plot_confusion_matrix(confusion_mtx, classes = range(10)) . Please access the code associated with this post. It’s self Explanatory along with the Literature given here. .",
            "url": "https://saptarshidatta.in/2020/08/31/Digit-Recognizer-CNN.html",
            "relUrl": "/2020/08/31/Digit-Recognizer-CNN.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "EDA with Titanic Dataset - Literature",
            "content": "The Titanic data set is a famous data set that beginners in Machine Learning always refer to. However, the data set presents some interesting EDA opportunities which can be applied towards further complex problems. . Problem Description . The sinking of the Titanic is one of the most infamous shipwrecks in history. . On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. . While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. . In this article, we perform an Exploratory Data Analysis(EDA) which will show trends on what sorts of people were more likely to survive. We will passenger data (ie name, age, gender, socio-economic class, etc) obtained from Kaggle. . Data Dictionary: . . Assumptions: . The following has been assumed with respect to some of the variables and further has been used to derive some additional variables for better analysis. . pclass: This variable is a proxy for socio-economic status (SES) . 1st = Upper . | 2nd = Middle . | 3rd = Lower . | . age: This variable provides information about the age of the passenger . Age is fractional if less than 1. . | If the age is estimated, is it in the form of xx.5 . | . sibsp: The variable defines family relations in this way… . Sibling = brother, sister, stepbrother, stepsister . | Spouse = husband, wife (mistresses and fiances have been ignored) . | . parch: This variable defines family relations in this way… . Parent = mother, father . | Child = daughter, son, stepdaughter, stepson . | Some children traveled only with a nanny, therefore parch=0 for them. . | . Feature Engineering . The following Feature Engineering tasks were done. . Expanded the acronym C, S, Q for the Embarked Variable with Cherbourg, Southampton &amp; Queenstown respectively . | Expanded the acronym 1,2,3 for the Pclass Variable with 1st Class, 2nd Class &amp; 3rd Class respectively . | determined whether a passenger is a minor depending on his/her age(age &lt;18) and stored that in a different variable. . | Created new variables age_group and age_range based on the passengers age. . | Determined whether a passenger is married or single from their name, age and sex. . | Determined Travel companion of the passengers out of Sibling/Spouse and Parent/Children data. . | Determined the fare_range from the fare variable. . | Filled the rows of Cabin Variable with ‘Not in a Cabin” which were earlier empty. Else we would have to remove this variable as only 23% of the passengers had a cabin. . | . Missing Value Treatment . The missing value in the Age variable has been imputed with mean age. | . Summary . Let’s start with the summary of ideas that will be presented in the notebook: . The data set from the data quality point of view has some missing data, but they do not impact the analysis that much as we have feature engineered these variables and also derived some other variables from these variables that resulted in a clear analysis. Although there is a discrepancy between the number of tickets and passengers(it looks like a single ticket was shared among multiple passengers) and the sum of the records (training and test datasets). There has also been no information regarding the crew members with their details, so nothing is known about them, but the goal of the analysis is to determine the factors why some passengers survived. . A number of interesting yet relatable insights were found: . more men passengers died than female passengers . | most adult men between the ages 18 and 65 did not survive (even though we have a percentage of data where the age is not known and we used missing value imputation.) . | first class passengers were given or automatically got priority over the other classes in getting rescued. . | Fare is a misleading feature and cannot be easily correlated to survival, as it is linked to few other features like Passenger class, Cabin and Ticket . | Passenger class, Cabin, Ticket and Fare together give a better picture of survival . | we know a number of passengers across all three passenger classes have overpaid for their tickets and we can easily point them. Maybe they bought tickets to board the ship in the black-market. . | a strange fact that ones who boarded from Southampton took the most of the death toll . | we found passengers passengers who have traveled with a ticket priced £0.00 across all three classes . | to sum it up, first class female passengers stood a better chance of surviving . | those that traveled with a company i.e. family or friend had a higher chance of survival than those who traveled by themselves, although ones with less able dependents suffered to a visible degree . | it appears that most men were helping women, children and elderly and other less able passengers to board off the ship and in turn minimized their own chances of survival, leading to 81% male casualties . | . Code . The Jupyter notebook for the entire exercise is available at this link, with options to view and run the code on Google Collab! . Please check it out. .",
            "url": "https://saptarshidatta.in/2020/08/02/EDA-with-Titanic-Data-set.html",
            "relUrl": "/2020/08/02/EDA-with-Titanic-Data-set.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Exploratory Data Analysis with Titanic Data Set",
            "content": "from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/gdrive . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline sns.set(style=&quot;whitegrid&quot;, font_scale=1.75) . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . train = pd.read_csv(&#39;/content/gdrive/My Drive/titanic/train.csv&#39;) test = pd.read_csv(&#39;/content/gdrive/My Drive/titanic/test.csv&#39;) print(train.shape) print(test.shape) . (891, 12) (418, 11) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . test_Id = test[&#39;PassengerId&#39;] . all_features = pd.concat((train, test), sort = False) . all_features.drop(&#39;PassengerId&#39;, axis =1, inplace = True) . Missing Value treatment . missing_values = (all_features.isnull().sum()/all_features.shape[0] *100).sort_values(ascending = False) missing_values . Cabin 77.463713 Survived 31.932773 Age 20.091673 Embarked 0.152788 Fare 0.076394 Ticket 0.000000 Parch 0.000000 SibSp 0.000000 Sex 0.000000 Name 0.000000 Pclass 0.000000 dtype: float64 . all_features[&#39;Age&#39;].fillna(all_features[&#39;Age&#39;].mean(), inplace = True) . Feature Engineering . def expand_embark_acronym(embarked): result = [] mapping = { &quot;C&quot;: &quot;Cherbourg&quot;, &quot;S&quot;: &quot;Southampton&quot;, &quot;Q&quot;: &quot;Queenstown&quot; } for each in embarked.values: if len(str(each)) &gt; 1: result.append(each) else: if each in mapping: result.append(mapping[each]) else: result.append(&quot;Unknown&quot;) return result def expand_pclass_acronym(pclass): result = [] mapping = { 1: &quot;1st class&quot;, 2: &quot;2nd class&quot;, 3: &quot;3rd class&quot; } for each in pclass.values: if len(str(each)) &gt; 1: result.append(each) else: if each in mapping: result.append(mapping[each]) else: result.append(&quot;Unknown&quot;) return result def is_a_minor(age): if age &lt; 18: return &quot;Under 18 (minor)&quot; return &quot;Adult&quot; # See https://help.healthycities.org/hc/en-us/articles/219556208-How-are-the-different-age-groups-defined- def apply_age_groups(age): result = [] mapping = { 1: &quot;Infant&quot;, # Infants: &lt;1 13: &quot;Child&quot;, # Children: &lt;18, &lt;11 or K - 7th grade 18: &quot;Teen&quot;, # Teens: 13-17 (Teens, who are not Adults) 66: &quot;Adult&quot;, # Adults: 20+ (includes adult teens: 18+) 123: &quot;Elderly&quot; # Elderly: 65+ (123 is the oldest age known till date) } for each_age in age.values: if type(each_age) == str: result.append(category) else: category = &quot;Unknown&quot; if each_age != np.nan: for each_age_range in mapping: if each_age &lt; each_age_range: category = mapping[each_age_range] break result.append(category) return result def apply_age_ranges(age): result = [] mapping = { 6: &quot;00-05 years&quot;, 12: &quot;06-11 years&quot;, 19: &quot;12-18 years&quot;, 31: &quot;19-30 years&quot;, 41: &quot;31-40 years&quot;, 51: &quot;41-50 years&quot;, 61: &quot;51-60 years&quot;, 71: &quot;61-70 years&quot;, 81: &quot;71-80 years&quot;, 91: &quot;81-90 years&quot;, 124: &quot;91+ years&quot;, # (123 is the oldest age known till date) } for each_age in age.values: if type(each_age) == str: result.append(category) else: category = &quot;Unknown&quot; if each_age != np.nan: for each_age_range in mapping: if each_age &lt; each_age_range: category = mapping[each_age_range] break result.append(category) return result def is_married_of_single(names, ages, sexes): result = [] for name, age, sex in zip(names.values, ages.values, sexes.values): if age &lt; 18: result.append(&quot;Not of legal age&quot;) else: if (&#39;Mrs.&#39; in name) or (&#39;Mme.&#39; in name): result.append(&quot;Married&quot;) elif (&#39;Miss.&#39; in name) or (&#39;Ms.&#39; in name) or (&#39;Lady&#39; in name) or (&#39;Mlle.&#39; in name): result.append(&quot;Single&quot;) else: result.append(&quot;Unknown&quot;) return result def apply_travel_companions(siblings_spouse, parent_children): result = [] for siblings_spouse_count, parent_children_count in zip(siblings_spouse.values, parent_children.values): if (siblings_spouse_count &gt; 0) and (parent_children_count &gt; 0): result.append(&quot;Parent/Children &amp; Sibling/Spouse&quot;) else: if (siblings_spouse_count &gt; 0): result.append(&quot;Sibling/Spouse&quot;) elif (parent_children_count &gt; 0): result.append(&quot;Parent/Children&quot;) else: result.append(&quot;Alone&quot;) return result def apply_fare_ranges(fare): result = [] mapping = { 11: &quot;£000 - 010&quot;, 21: &quot;£011 - 020&quot;, 41: &quot;£020 - 040&quot;, 81: &quot;£041 - 080&quot;, 101: &quot;£081 - 100&quot;, 201: &quot;£101 - 200&quot;, 301: &quot;£201 - 300&quot;, 401: &quot;£301 - 400&quot;, 515: &quot;£401 &amp; above&quot; # in this case the max fare is around £512 } for each_fare in fare.values: if type(each_fare) == str: result.append(category) else: category = &quot;Unknown&quot; if each_fare != np.nan: for each_fare_range in mapping: if each_fare &lt; each_fare_range: category = mapping[each_fare_range] break result.append(category) return result def were_in_a_cabin_or_not(row): if type(row) is str: return &quot;In a Cabin&quot; return &quot;Not in a Cabin&quot; . all_features[&#39;Embarked&#39;] = expand_embark_acronym(all_features[&#39;Embarked&#39;]) # Pclass: Passenger Class all_features[&#39;Pclass&#39;] = expand_pclass_acronym(all_features[&#39;Pclass&#39;]) # Age all_features[&#39;Adult_or_minor&#39;] = all_features[&#39;Age&#39;].apply(is_a_minor) females_filter = all_features[&#39;Sex&#39;] == &#39;female&#39; adult_filter = all_features[&#39;Adult_or_minor&#39;] == &#39;2. Adult&#39; all_features[&#39;Marital_status&#39;] = is_married_of_single(all_features[&#39;Name&#39;], all_features[&#39;Age&#39;], all_features[&#39;Sex&#39;]) all_features[&#39;Age_group&#39;] = apply_age_groups(all_features[&#39;Age&#39;]) all_features[&#39;Age_ranges&#39;] = apply_age_ranges(all_features[&#39;Age&#39;]) # SibSp and Parch: Sibling/Spouse counts, Parent/Children counts all_features[&#39;Travel_companion&#39;] = apply_travel_companions(all_features[&#39;SibSp&#39;], all_features[&#39;Parch&#39;]) # Fare: ticket fare across the different classes all_features[&#39;Fare_range&#39;] = apply_fare_ranges(all_features[&#39;Fare&#39;]) # Cabin: ticket holder has a cabin or not all_features[&#39;In_Cabin&#39;] = all_features[&#39;Cabin&#39;].apply(were_in_a_cabin_or_not) all_features[&#39;Cabin&#39;] = all_features[&#39;Cabin&#39;].fillna(&#39;No cabin&#39;) . all_features.head() . Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Adult_or_minor Marital_status Age_group Age_ranges Travel_companion Fare_range In_Cabin . 0 0.0 | 3rd class | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | No cabin | Southampton | Adult | Unknown | Adult | 19-30 years | Sibling/Spouse | £000 - 010 | Not in a Cabin | . 1 1.0 | 1st class | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | Cherbourg | Adult | Married | Adult | 31-40 years | Sibling/Spouse | £041 - 080 | In a Cabin | . 2 1.0 | 3rd class | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | No cabin | Southampton | Adult | Single | Adult | 19-30 years | Alone | £000 - 010 | Not in a Cabin | . 3 1.0 | 1st class | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | Southampton | Adult | Married | Adult | 31-40 years | Sibling/Spouse | £041 - 080 | In a Cabin | . 4 0.0 | 3rd class | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | No cabin | Southampton | Adult | Unknown | Adult | 31-40 years | Alone | £000 - 010 | Not in a Cabin | . missing_values = (all_features.isnull().sum()/all_features.shape[0] *100).sort_values(ascending = False) missing_values . Survived 31.932773 Embarked 0.152788 Fare 0.076394 Age_ranges 0.000000 Age_group 0.000000 Marital_status 0.000000 Adult_or_minor 0.000000 Travel_companion 0.000000 Cabin 0.000000 Fare_range 0.000000 Ticket 0.000000 Parch 0.000000 SibSp 0.000000 Age 0.000000 Sex 0.000000 Name 0.000000 Pclass 0.000000 In_Cabin 0.000000 dtype: float64 . def passenger_stats(dataset): total_ticket_holders = dataset.shape[0] siblings_count = dataset[&#39;SibSp&#39;].sum() parents_children_count = dataset[&#39;Parch&#39;].sum() print(&quot;total_ticket_holders:&quot;, total_ticket_holders) print(&quot;siblings_count:&quot;, siblings_count) print(&quot;parents_children_count:&quot;, parents_children_count) print(&quot;total (siblings, parents and children count):&quot;, siblings_count + parents_children_count) grand_total = total_ticket_holders + siblings_count + parents_children_count print(&quot;grand total (ticket holders, siblings, parents, children count):&quot;, grand_total) return grand_total training_dataset_passengers_count = passenger_stats(all_features) . total_ticket_holders: 1309 siblings_count: 653 parents_children_count: 504 total (siblings, parents and children count): 1157 grand total (ticket holders, siblings, parents, children count): 2466 . Creating the test &amp; train dataset again. . train = all_features[: 891] test = all_features[891:] . print(train.shape) print(test.shape) . (891, 18) (418, 18) . missing_values = (test.isnull().sum()/test.shape[0] *100).sort_values(ascending = False) missing_values . Survived 100.000000 Fare 0.239234 Travel_companion 0.000000 Age_ranges 0.000000 Age_group 0.000000 Marital_status 0.000000 Adult_or_minor 0.000000 Embarked 0.000000 Cabin 0.000000 Fare_range 0.000000 Ticket 0.000000 Parch 0.000000 SibSp 0.000000 Age 0.000000 Sex 0.000000 Name 0.000000 Pclass 0.000000 In_Cabin 0.000000 dtype: float64 . Remove the Survived Variable from the test dataset since it&#39;s empty ans the test dataset shouldn&#39;t contain the Target Variable. . test.drop(&#39;Survived&#39;,axis = 1, inplace = True) . /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, . Analysis . Distribution of the dataset . g = sns.countplot(train[&#39;Survived&#39;]) plt.legend(loc=&#39;upper right&#39;) g.set(xlabel=&quot;Survival&quot;, xticklabels=[&quot;Died&quot;, &quot;Survived&quot;]) # &quot;0=Died&quot;, &quot;1=Survived&quot; . No handles with labels found to put in legend. . [[Text(0, 0, &#39;Died&#39;), Text(0, 0, &#39;Survived&#39;)], Text(0.5, 0, &#39;Survival&#39;)] . Sex of the passenger associated with Survival . g = sns.countplot(train[&#39;Survived&#39;], hue = train[&#39;Sex&#39;]) plt.legend(loc=&#39;upper right&#39;) g.set(xlabel=&quot;Survival&quot;, xticklabels=[&quot;Died&quot;, &quot;Survived&quot;]) . [[Text(0, 0, &#39;Died&#39;), Text(0, 0, &#39;Survived&#39;)], Text(0.5, 0, &#39;Survival&#39;)] . Passenger Class associated with survival . g = sns.countplot(train[&#39;Survived&#39;], hue = train[&#39;Pclass&#39;]) plt.legend(loc=&#39;upper right&#39;) g.set(xlabel=&quot;Survival&quot;, xticklabels=[&quot;Died&quot;, &quot;Survived&quot;]) . [[Text(0, 0, &#39;Died&#39;), Text(0, 0, &#39;Survived&#39;)], Text(0.5, 0, &#39;Survival&#39;)] . gender_class_pivot = train.pivot_table(values=[&#39;Survived&#39;], index=[&#39;Sex&#39;, &#39;Pclass&#39;]) gender_class_pivot . Survived . Sex Pclass . female 1st class 0.968085 | . 2nd class 0.921053 | . 3rd class 0.500000 | . male 1st class 0.368852 | . 2nd class 0.157407 | . 3rd class 0.135447 | . g = sns.catplot(x=&quot;Survived&quot;, hue=&quot;Pclass&quot;, col=&#39;Sex&#39;, data=train.sort_values(by=&#39;Pclass&#39;), kind=&#39;count&#39;) g.set(xticklabels=[&#39;Died&#39;, &#39;Survived&#39;], xlabel=&quot;Survival&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fea557bce10&gt; . 3rd Class male passengers forms the largest group who died. In all the three classes femely passengers survived the most. . Fares Paid associated with Survival . g = sns.catplot(x=&quot;Survived&quot;, y=&quot;Fare&quot;, hue=&quot;Pclass&quot;, data=train.sort_values(by=&#39;Pclass&#39;), kind=&quot;bar&quot;); g.set(xticklabels=[&#39;Died&#39;, &#39;Survived&#39;], xlabel=&quot;Survival&quot;, title=&quot;Sum of fares collected across the three Passenger Classes and Survival&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fea55d7a2e8&gt; . g = sns.countplot(train[&#39;Pclass&#39;]) plt.legend(loc=&#39;upper right&#39;) g.set(xlabel=&quot;Passenger Class&quot;) . No handles with labels found to put in legend. . [Text(0.5, 0, &#39;Passenger Class&#39;)] . Though the 3rd Class passengers forms the largest group but it&#39;s the 1st Class passengers who survived the most. This shows Rescue services were provided to wealthy passengers. . Passenger fare range with survival . def fare_range_with_survival( passenger_class, title): dataset = train.copy() class_filter = dataset[&#39;Pclass&#39;] == passenger_class dataset = dataset[class_filter] dataset[class_filter] g = sns.catplot(y=&quot;Fare_range&quot;, hue=&quot;Survived&quot;, data=dataset.sort_values(by=&#39;Pclass&#39;), kind=&quot;count&quot;) g.set(ylabel=&quot;Fare range&quot;, title=title) new_labels = [&#39;Died&#39;, &#39;Survived&#39;] for t, l in zip(g._legend.texts, new_labels): t.set_text(l) g.fig.set_figwidth(35) . fare_range_with_survival(&#39;1st class&#39;, &quot;First class passengers and Fare ranges&quot;) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index. . fare_range_with_survival(&#39;2nd class&#39;, &quot;Second class passengers and Fare ranges&quot;) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index. . fare_range_with_survival(&#39;3rd class&#39;, &quot;Third class passengers and Fare ranges&quot;) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index. . Survival with age . g = sns.catplot(col=&quot;Sex&quot;, x=&quot;Survived&quot;, hue=&quot;Age_group&quot;, data=train.sort_values(by=&#39;Age&#39;), kind=&#39;count&#39;) g.set(xlabel=&quot;Survival&quot;, xticklabels=[&#39;Died&#39;, &#39;Survived&#39;]) g.fig.set_figwidth(20) . g = sns.catplot(y=&quot;Age&quot;, x=&quot;Sex&quot;, hue=&quot;Survived&quot;, data=train) new_labels = [&#39;Died&#39;, &#39;Survived&#39;] for t, l in zip(g._legend.texts, new_labels): t.set_text(l) g.fig.set_figwidth(16) . Survival with Travel Companion . sibling_spouse_pivot_table = train.pivot_table(values = [&#39;Survived&#39;],index = &#39;SibSp&#39;) sibling_spouse_pivot_table sibling_spouse_pivot_table.plot(kind=&#39;barh&#39;) plt.ylabel(&#39;Sibling/spouse count&#39;) . Text(0, 0.5, &#39;Sibling/spouse count&#39;) . parent_children_pivot_table = train.pivot_table(values = [&#39;Survived&#39;],index = &#39;SibSp&#39;) parent_children_pivot_table sibling_spouse_pivot_table.plot(kind=&#39;barh&#39;) plt.ylabel(&#39;Parent/children count&#39;) . Text(0, 0.5, &#39;Parent/children count&#39;) . travel_companion_pivot_table = train.pivot_table(values = [&#39;Survived&#39;],index = &#39;Travel_companion&#39;) travel_companion_pivot_table travel_companion_pivot_table.plot(kind=&#39;barh&#39;) plt.ylabel(&#39;Travel Companion&#39;) . Text(0, 0.5, &#39;Travel Companion&#39;) . adult_or_minor_pivot_table = train.pivot_table(values=[&#39;Survived&#39;], index=[&#39;Adult_or_minor&#39;, &#39;Sex&#39;]) adult_or_minor_pivot_table adult_or_minor_pivot_table.plot(kind=&#39;barh&#39;) plt.ylabel(&#39;Adult/ Minor&#39;) . Text(0, 0.5, &#39;Adult/ Minor&#39;) . Embarked with survival . embarked_pivot_table=train.pivot_table(values=[&#39;Survived&#39;], index=&#39;Embarked&#39;) embarked_pivot_table . Survived . Embarked . Cherbourg 0.553571 | . Queenstown 0.389610 | . Southampton 0.336957 | . It looks like, Passengers who boarded the Titanic from Southampton were the least fornunate. We need to analyze more. Let&#39;s do it with Passenger Class. . embarked_passenger_class_pivot_table = train.pivot_table(values=[&#39;Survived&#39;], index=[&#39;Embarked&#39;, &#39;Pclass&#39;]) embarked_passenger_class_pivot_table . Survived . Embarked Pclass . Cherbourg 1st class 0.694118 | . 2nd class 0.529412 | . 3rd class 0.378788 | . Queenstown 1st class 0.500000 | . 2nd class 0.666667 | . 3rd class 0.375000 | . Southampton 1st class 0.582677 | . 2nd class 0.463415 | . 3rd class 0.189802 | . 3rd Class passengers who boarded from Souththampon, were the least fortunate. . embarked_passenger_class_pivot_table.plot(kind = &#39;barh&#39;) plt.ylabel(&#39;Embarkeded/ Passenger Class&#39;) . Text(0, 0.5, &#39;Embarkeded/ Passenger Class&#39;) . Cabin &amp; Passenger Class with Survival . g = sns.catplot(col=&quot;In_Cabin&quot;, x=&#39;Pclass&#39;, hue=&quot;Survived&quot;, kind=&quot;count&quot;, data=train.sort_values(by=&#39;Pclass&#39;)); new_labels = [&#39;Died&#39;, &#39;Survived&#39;] for t, l in zip(g._legend.texts, new_labels): t.set_text(l) g.fig.set_figwidth(16) g.set(xlabel=&quot;Passenger Class&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7fea512efba8&gt; . It looks like being in a cabin, somehow helped with Survival . Tickets . train[&#39;Ticket&#39;].describe() . count 891 unique 681 top 1601 freq 7 Name: Ticket, dtype: object . There are 681 unique tickets out of 891 passengers. It seems some passengers shared a single ticket. . train[&#39;Ticket&#39;].value_counts() . 1601 7 CA. 2343 7 347082 7 3101295 6 347088 6 .. 36568 1 6563 1 19947 1 PC 17597 1 350035 1 Name: Ticket, Length: 681, dtype: int64 . The point of sharing a single ticket by many passengers is shown above. .",
            "url": "https://saptarshidatta.in/2020/08/01/Titanic-Data-EDA.html",
            "relUrl": "/2020/08/01/Titanic-Data-EDA.html",
            "date": " • Aug 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Knowledge Base",
          "content": "Concepts . Missing Value Treatment | Data Scaling &amp; Normalization | Hypothesis Testing | Parameter Optimization in Neural Networks | Optimizer | Search Strategies | Articles by other Authors . Why and how residual blocks work . | Batch normalization and dropout explained . | One Cycle Learning Rate Policy . | Weight decay . | Gradient clipping . | Logic and Proof . | . IIT Jodhpur MTech(Artificial Intelligence) . Semester 1 : https://github.com/saptarshidatta96/MTech_Sem1 . Classical Machine Learning . Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. It is the science of getting computers to act without being explicitly programmed. . Here are a few materials: . Statistics Book - Business Statistics - A First Course by David M. Levine, David F. Stephan, Kathryn A. Szabat, P.K. Viswanatha . | Introduction to Probability and Statistics( For Scientists and Engineers) . | Head First Statistics by Dawn Griffiths - Available at Safari Books Online . | YouTube Channel - StatQuest with Josh Starmer is a good place to start. . | Introduction to Statistical Learning in R (ISLR) by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani . | YouTube video series of ISLR . | The Hundred-Page Machine Learning Book by Andriy Burkov. This book will provide a concise understanding of various ML Algorithms. It comes with a wiki with additional information like Q&amp;A, code snippets, further reading, tools, and other relevant resources. Having a prior understanding of various ML Techniques is advised. . | Python for Data Science. Taking some Udemy course will help. . | . Deep Learning . Deep Learning is a sub field of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. . Here are a few materials: . Deep Learning Book by Ian Goodfellow, Yoshua Bengio and Aaron Courville . | Deep Learning Specialization by Andrew Ng from Coursera . | Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition . | DS-GA 1008 · SPRING 2020 · NYU CENTER FOR DATA SCIENCE - Link . | YouTube Playlist - Essence of Linear Algebra . | YouTube Playlist - MIT 18.06 Linear Algebra, Spring 2005 / Course Website . | DeepLizard.com provides end to end high quality tutorials on Deep Learing with Keras, TensorFlow and PyTorch. Make sure to watch it. . | Paperspace Blog . | YouTube playlist on Evolution of Object Detection Networks . | .",
          "url": "https://saptarshidatta.in/kb/",
          "relUrl": "/kb/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Projects",
          "content": "Sign Language Prediction with MobileNet . In this exercise we shall again apply transfer learning to predict the Numeric Sign Language. We will be applying MobileNet Model and shall modify the model and then fine tune it to suit our requirements. Click here to view the code. . Tools | . TensorFlow, Transfer Learning, CNN, Google Collab Notebook . MNIST Digit Recognizer using CNN . MNIST (“Modified National Institute of Standards and Technology”) is the de-facto “hello world” data set of computer vision. Since its release in 1999, this classic data set of handwritten images has served as the basis for bench marking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. In this exercise, our goal is to correctly identify digits from a data set of tens of thousands of handwritten images. Click here to view the code. . Tools | . TensorFlow, CNN, Google Collab Notebooks . Exploratory Data Analysis - Titanic data set(Kaggle) . The objective of this exercise was to clean the data and then apply feature engineering to generate meaningful insights of the Titanic data set available at Kaggle . A training data set with 891 rows was used for this exercise. The data set has interesting features like age, gender, fare etc of the passengers to predict whether they survives the Titanic mishap. Click here to view the code. . Tools | . Missing Value Treatment, Feature Engineering and Exploratory Data Analysis, Google Colab Notebooks . Creation of an India Credit Risk Default Model Using Logistic Regression . The project involved developing a credit risk default model using a given data which had to be checked for outliers, missing values, multicollinearity etc. Univariate and Bivariate Analysis had to be conducted and the model had to be built using Logistic Regression on most important variables. Model Performance Measures were undertaken that included predicting the accuracy of the model on certain datasets. Click here to view the code. . Tools | . Logistic Regression, Univariate &amp; Bivariate Analysis, Outlier Treatment, Model Performance Measures . Visualizing Car Insurance Claims using Tableau . This project explored the art of problem-solving with the aid of visual analytics. Tableau’s data visualization tools were used to create interactive dashboards to provide high-level insights to an Insurance company to drive the company’s car insurance schemes. . Tools | . Data Visualization, Tableau, Business Intelligence . Build a forecasting model to predict monthly gas production . The project involved developing an ARIMA model to forecast the monthly Australian gas production level for the next 12 months. Click here to view the Code. . Tools | . ARIMA, Time Series Forecasting, ADF Test . Choosing preferable mode of transport by employees . The project involves deciding on the mode of transport that the employees prefer while commuting to office. For this, multiple models such as KNN, Naive Bayes, Logistic Regression have been created and explored to check their model performance metrics. Bagging and Boosting modelling procedures have also been applied to create the models. Click here to view the code. . Tools | . Bagging and Boosting, KNN, Naive Bayes, Logistic Regression . Cellphone-Logistic project . The primary objective was to investigate the parameters contributing for customer churn (attrition) in the Telecom Industry. A Logistic Regression Model was developed and validated with test data to predict customer churn. . -Tools . Logistic Regression, Model Comparison, Predictive Analytics . Building a supervised Model to cross-sell personal loans The objective of this exercise was to build a model using a Supervised learning technique to figure out profitable segments to target for cross-selling personal loans. A Pilot campaign data of 20000 customers was used which included several demographic and behavioral variables. The Model was further validated and a deployment strategy was recommended. . Tools | . Random Forest, Data Mining, Pruning, Model Performance Measures .",
          "url": "https://saptarshidatta.in/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "",
          "content": "",
          "url": "https://saptarshidatta.in/cv/",
          "relUrl": "/cv/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Curriculum Vitae",
          "content": "Saptarshi Datta . datta.3@iitj.ac.in . LinkedIn | GitHub | Website | . Career Objective . To work in a challenging and motivating career that provides an opportunity to work with the latest technologies. . SKILLS . Machine Learning: Classification, Regression, Clustering, Decision Trees, Random Forests . Deep Learning: Forward Propagation, Backward propagation, Hyper parameter Tuning, Regularization &amp; Optimization, CNN &amp; RNN models. Experience with Keras API (Tensorflow Backend) snd PyTorch. . Programming Languages: Python . Scripting Language: Unix . Database Language: SQL . Data Extraction Tool: Informatica PowerCenter, IBM Infosphere DataStage . Data Reporting Tool: Tableau . EXPERIENCE . Tata Consultancy Services, Kolkata - Systems Engineer (November 2018 - PRESENT) . Data Engineer - Analytics and Reporting platform (February, 2019 - July, 2021) . | AI/ML Developer - TCS Rapid Labs (July, 2021 - Present) . | . CERTIFICATIONS . DP 900 - Microsoft Azure Data Fundamentals . | Python for Data Science and Machine Learning Bootcamp, Udemy . | Deep Learning Specialization, Coursera . | . EDUCATION . Indian Institute of Technology, Jodhpur . MTech - Artificial Intelligence, part-time degree . (February 2021 – till date) . | Great Lakes Institute of Management . Post Graduate Program - Business Analytics &amp; Business Intelligence . (May 2019 – May 2020) . | Maulana Abul Kalam Azad University of Technology, West Bengal . BTech - Electrical Engineering, DGPA: 8.46/10 . (August 2014 – June 2018) . | .",
          "url": "https://saptarshidatta.in/cv/",
          "relUrl": "/cv/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "About Me",
          "content": "Hi, I am Saptarshi Datta. I am currently working as AI/ML Developer at TCS Rapid Labs. . I have a Bachelor’s degree in Electrical Engineering and completed the Post Graduate Program in Business Analytics &amp; Business Intelligence from UT Austin. . I am also pursing part-time MTech in Artificial Intelligence from IIT Jodhpur. . I am passionate about a lot of things and number crunching is one of them. Apart from this, I love to blog. In my free time, I like to relax a bit! :) . To know more about me, please feel free to have a look at my CV. .",
          "url": "https://saptarshidatta.in/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://saptarshidatta.in/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}