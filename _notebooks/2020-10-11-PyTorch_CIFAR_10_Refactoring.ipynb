{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch CIFAR-10 Refactoring.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8U-WGj90Jl7"
      },
      "source": [
        "# CNN Training Loop Refactoring (Simultaneous Hyperparameter Testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC6QAjtSpLm6"
      },
      "source": [
        "When we [last](https://saptarshidatta.in/2020/10/06/PyTorch_CIFAR10_TB.html) trained our network, we built out quite a lot of functionality that allowed us to experiment with many different parameters and values, and we also made the calls need inside our training loop that would get our results into TensorBoard.\n",
        "\n",
        "All of this work has helped, but our training loop is quite crowded now. In this exercise, we're going to clean up our training loop and set the stage for more experimentation up by using the `RunBuilder` class that we built last time and by building a new class called `RunManager`.\n",
        "\n",
        "I also find this way of Hyperparameter Tuning more intuitive than TensorBoard. Also, as our number of parameters and runs get larger, TensorBoard will start to breakdown as a viable solution for reviewing our results.\n",
        "\n",
        "However, calls have been made inside our `RunManager` class to TensorBoard, so it can be used as an added functionality. For reference, on how to use TensorBoard with PyTorch inside Google Collab, plese refer [here](https://saptarshidatta.in/2020/10/06/PyTorch_CIFAR10_TB.html).\n",
        "\n",
        "The code also generates results in `csv` and `json` format, which can be used gor further analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28sB7uXPimG8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from IPython.display import display, clear_output\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "\n",
        "from itertools import product\n",
        "from collections import namedtuple\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIoI5SyHtAHH"
      },
      "source": [
        "## Designing the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeGzwSVqipb1"
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "    self.out = nn.Linear(in_features=84, out_features=10)\n",
        "\n",
        "  def forward(self, t):\n",
        "    #Layer 1\n",
        "    t = t\n",
        "    #Layer 2\n",
        "    t = self.conv1(t)\n",
        "    t = F.relu(t)\n",
        "    t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (6,14,14)\n",
        "    #Layer 3\n",
        "    t = self.conv2(t)\n",
        "    t = F.relu(t)\n",
        "    t = F.max_pool2d(t, kernel_size=2, stride=2)#output shape : (16,5,5)\n",
        "    #Layer 4\n",
        "    t = t.reshape(-1, 16*5*5)\n",
        "    t = self.fc1(t)\n",
        "    t = F.relu(t)#output shape : (1,120)\n",
        "    #Layer 5\n",
        "    t = self.fc2(t)\n",
        "    t = F.relu(t)#output shape : (1, 84)\n",
        "    #Layer 6/ Output Layer\n",
        "    t = self.out(t)#output shape : (1,10)\n",
        "\n",
        "    return t"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyhvwLLar0YM"
      },
      "source": [
        "## `RunBuilder` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8MekfxKi0v6"
      },
      "source": [
        "class RunBuilder():\n",
        "    @staticmethod\n",
        "    def get_runs(params):\n",
        "\n",
        "        Run = namedtuple('Run', params.keys())\n",
        "\n",
        "        runs = []\n",
        "        for v in product(*params.values()):\n",
        "            runs.append(Run(*v))\n",
        "\n",
        "        return runs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_JggKs1sG30"
      },
      "source": [
        "## `RunManager` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8opwcSpi7vy"
      },
      "source": [
        "class RunManager():\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.epoch_count = 0\n",
        "        self.epoch_loss = 0\n",
        "        self.epoch_num_correct = 0\n",
        "        self.epoch_start_time = None\n",
        "        \n",
        "        self.run_params = None\n",
        "        self.run_count = 0\n",
        "        self.run_data = []\n",
        "        self.run_start_time = None\n",
        "        \n",
        "        self.network = None\n",
        "        self.loader = None\n",
        "        self.tb = None\n",
        "        \n",
        "    def begin_run(self, run, network, loader):\n",
        "        \n",
        "        self.run_start_time = time.time()\n",
        "\n",
        "        self.run_params = run\n",
        "        self.run_count += 1\n",
        "        \n",
        "        self.network = network\n",
        "        self.loader = loader\n",
        "        self.tb = SummaryWriter(comment=f'-{run}')\n",
        "        \n",
        "        images, labels = next(iter(self.loader))\n",
        "        grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "        self.tb.add_image('images', grid)\n",
        "        self.tb.add_graph(\n",
        "             self.network\n",
        "            ,images.to(getattr(run, 'device', 'cpu'))\n",
        "        )\n",
        "        \n",
        "    def end_run(self):\n",
        "        self.tb.close()\n",
        "        self.epoch_count = 0   \n",
        "\n",
        "    def begin_epoch(self):\n",
        "        self.epoch_start_time = time.time()\n",
        "        \n",
        "        self.epoch_count += 1\n",
        "        self.epoch_loss = 0\n",
        "        self.epoch_num_correct = 0\n",
        "\n",
        "    def end_epoch(self):\n",
        "        \n",
        "        epoch_duration = time.time() - self.epoch_start_time\n",
        "        run_duration = time.time() - self.run_start_time\n",
        "        \n",
        "        loss = self.epoch_loss / len(self.loader.dataset)\n",
        "        accuracy = self.epoch_num_correct / len(self.loader.dataset)\n",
        "                \n",
        "        self.tb.add_scalar('Loss', loss, self.epoch_count)\n",
        "        self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)\n",
        "        \n",
        "        for name, param in self.network.named_parameters():\n",
        "            self.tb.add_histogram(name, param, self.epoch_count)\n",
        "            self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)\n",
        "        \n",
        "        results = OrderedDict()\n",
        "        results[\"run\"] = self.run_count\n",
        "        results[\"epoch\"] = self.epoch_count\n",
        "        results['loss'] = loss\n",
        "        results[\"accuracy\"] = accuracy\n",
        "        results['epoch duration'] = epoch_duration\n",
        "        results['run duration'] = run_duration\n",
        "        for k,v in self.run_params._asdict().items(): results[k] = v\n",
        "        self.run_data.append(results)\n",
        "        \n",
        "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        display(df)\n",
        "        \n",
        "    def track_loss(self, loss, batch):\n",
        "        self.epoch_loss += loss.item() * batch[0].shape[0]\n",
        "        \n",
        "    def track_num_correct(self, preds, labels):\n",
        "        self.epoch_num_correct += self._get_num_correct(preds, labels)\n",
        "    \n",
        "    def _get_num_correct(self, preds, labels):\n",
        "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
        "    \n",
        "    def save(self, fileName):\n",
        "        \n",
        "        pd.DataFrame.from_dict(\n",
        "            self.run_data\n",
        "            ,orient='columns'\n",
        "        ).to_csv(f'{fileName}.csv')\n",
        "        \n",
        "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWgRpmrscfT"
      },
      "source": [
        "## Loading the CIFAR-10 data and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl2SMDGVi_OK",
        "outputId": "c6b24208-acab-41cd-9b28-43bd7d05cc6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "transform =transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root='./data'\n",
        "    ,train=True\n",
        "    ,download=True\n",
        "    ,transform=transform\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVvHIScdsM5j"
      },
      "source": [
        "## Training the Nueral Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxpoZAwyjAim",
        "outputId": "eb650cf8-8f7a-47d3-9836-5dabeb779b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "params = OrderedDict(\n",
        "    lr = [.01]\n",
        "    ,batch_size = [1000]\n",
        "    ,shuffle = [True]\n",
        "    ,num_workers = [0, 1, 2, 4, 8, 16]\n",
        ")\n",
        "m = RunManager()\n",
        "for run in RunBuilder.get_runs(params):\n",
        "\n",
        "    network = Network()\n",
        "    loader = DataLoader(train_set, batch_size=run.batch_size, shuffle=run.shuffle, num_workers=run.num_workers)\n",
        "    optimizer = optim.Adam(network.parameters(), lr=run.lr)\n",
        "    \n",
        "    m.begin_run(run, network, loader)\n",
        "    for epoch in range(1):\n",
        "        m.begin_epoch()\n",
        "        for batch in loader:\n",
        "            \n",
        "            images, labels = batch\n",
        "            preds = network(images) # Pass Batch\n",
        "            loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
        "            optimizer.zero_grad() # Zero Gradients\n",
        "            loss.backward() # Calculate Gradients\n",
        "            optimizer.step() # Update Weights\n",
        "            \n",
        "            m.track_loss(loss, batch)\n",
        "            m.track_num_correct(preds, labels)  \n",
        "        m.end_epoch()\n",
        "    m.end_run()\n",
        "m.save('results')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>epoch</th>\n",
              "      <th>loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>epoch duration</th>\n",
              "      <th>run duration</th>\n",
              "      <th>lr</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>shuffle</th>\n",
              "      <th>num_workers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.869639</td>\n",
              "      <td>0.30556</td>\n",
              "      <td>18.566388</td>\n",
              "      <td>20.198124</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.967650</td>\n",
              "      <td>0.26044</td>\n",
              "      <td>16.305351</td>\n",
              "      <td>18.226968</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.930892</td>\n",
              "      <td>0.27954</td>\n",
              "      <td>15.927075</td>\n",
              "      <td>18.112834</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1.850479</td>\n",
              "      <td>0.30830</td>\n",
              "      <td>16.800490</td>\n",
              "      <td>19.740744</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.822490</td>\n",
              "      <td>0.32622</td>\n",
              "      <td>17.848449</td>\n",
              "      <td>21.122425</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.865499</td>\n",
              "      <td>0.30920</td>\n",
              "      <td>19.867091</td>\n",
              "      <td>24.908000</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   run  epoch      loss  accuracy  ...    lr  batch_size  shuffle  num_workers\n",
              "0    1      1  1.869639   0.30556  ...  0.01        1000     True            0\n",
              "1    2      1  1.967650   0.26044  ...  0.01        1000     True            1\n",
              "2    3      1  1.930892   0.27954  ...  0.01        1000     True            2\n",
              "3    4      1  1.850479   0.30830  ...  0.01        1000     True            4\n",
              "4    5      1  1.822490   0.32622  ...  0.01        1000     True            8\n",
              "5    6      1  1.865499   0.30920  ...  0.01        1000     True           16\n",
              "\n",
              "[6 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_kqfMJJppzH"
      },
      "source": [
        "## Experimenting with DataLoader `num_workers` attribute\n",
        "\n",
        "The `num_workers` attribute tells the data loader instance how many sub-processes to use for data loading. By default, the `num_workers` value is set to zero, and a value of zero tells the loader to load the data inside the main process.\n",
        "\n",
        "This means that the training process will work sequentially inside the main process. After a batch is used during the training process and another one is needed, we read the batch data from disk.\n",
        "\n",
        "Now, if we have a worker process, we can make use of the facility that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. The batches are loaded using additional worker processes and are queued up in memory.\n",
        "\n",
        "The main take-away from these results is that having a single worker process in addition to the main process resulted in a speed up of about twenty percent. However, adding additional worker processes after the first one didn't really show any further improvements.\n",
        "\n",
        "Additionally, we can see with higher number of num_workers results in higher run times. Please go through this [link](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/5) to know more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_aSpj25uKtd"
      },
      "source": [
        "## Summary\n",
        "\n",
        "We have introduced a way to experiment with Hyperparameters to extract maximum efficiency for our model. This code can be scaled up or scaled down to change the Hyperparameters we wish to experiment upon. \n",
        "\n",
        "This may be noted that, accuracy is not that high as we have trained our model for 1 epoch with each set of parameters. This has been purely done for experimentation purpose.\n",
        "\n",
        "However, we might need to change our network architecture i.e. a deeper network for higher efficiency."
      ]
    }
  ]
}